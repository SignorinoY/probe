\chapter{Wishart and Laguerre Ensembles}

Suppose $\left\{\mathbf{X}\right\}$ be a sequence of random vectors defined in $\mathbb{R}^{n}$, such that
\begin{equation*}
    E\left(\mathbf{X}\right)=0,\quad E\left(\mathbf{X}\otimes\mathbf{X}\right)=\mathrm{I}_{n}
\end{equation*}
and let $\left(X_{i}\right)_{1\leq i\leq n}$ be the components of the random vector $\mathbf{X}$.

Suppose $\left\{m_{n}\right\}$ be a sequence defined in $\mathbb{N}$ such that
\begin{equation*}
    0<\underline{\rho}:=\liminf_{n\rightarrow\infty}\frac{n}{m_{n}}\leq\limsup_{n\rightarrow\infty}\frac{n}{m_{n}}=:\bar{\rho}<\infty
\end{equation*}

Let $\mathbf{X}^{(1)},\ldots,\mathbf{X}^{\left(m_{n}\right)}$ be i.i.d. copies of $\mathbf{X}$, and $\mathbb{X}$ be the $m_{n}\times n$ random matrix with i.i.d. rows $\mathbf{X}^{(1)},\ldots,\mathbf{X}^{\left(m_{n}\right)}$, and their empirical covariance matrix is
\begin{equation*}
    \widehat{\boldsymbol{\Sigma}}:=\frac{1}{m_{n}}\sum_{k=1}^{m_{n}}\mathbf{X}^{(k)}\otimes \mathbf{X}^{(k)}=\frac{1}{m_{n}}\mathbb{X}^{\prime}\mathbb{X}
\end{equation*}
which is a $n\times n$ symmetric positive semidefinite random matrix, and
\begin{equation*}
    E\left(\widehat{\boldsymbol{\Sigma}}\right)=\mathbb{E}\left(\mathbf{X}\otimes\mathbf{X}\right)=\mathrm{I}_{n}
\end{equation*}
and $\widehat{\boldsymbol{\Sigma}}$ is also called Wishart matrix.

For convenience, we define the random matrix
\begin{equation*}
    \mathbf{A}:=m_{n}\widehat{\boldsymbol{\Sigma}}=\mathbb{X}^{\prime}\mathbb{X}=\sum_{k=1}^{m_{n}}\mathbf{X}^{(k)}\otimes\mathbf{X}^{(k)}
\end{equation*}

\begin{note}
    The eigenvalues of $\mathbf{A}$ are squares of the singular values of $\mathbb{X}$, in particularly
    \begin{equation*}
        \lambda_{\max}\left(\mathbf{A}\right)=s_{\max}\left(\mathbb{X}\right)^{2}=\max_{\|\mathbf{x}\|=1}\left\|\mathbb{X}\mathbf{x}\right\|^{2}=\left\|\mathbb{X}\right\|_{2}^{2}
    \end{equation*}
    if $m_{n}\geq n$, then
    \begin{equation*}
        \lambda_{\min}\left(\mathbf{A}\right)=s_{\min}\left(\mathbb{X}\right)^{2}=\min_{\|\mathbf{x}\|=1}\left\|\mathbb{X}\mathbf{x}\right\|^{2}=\left\|\mathbb{X}^{-1}\right\|_{2}^{-2}
    \end{equation*}
\end{note}

\section{Wishart Distribution}

Specially, if the random variables $\left(X_{i}\right)_{1\leq i\leq n}$ are i.i.d. standard Gaussians, then the distribution of the random matrix $\widehat{\boldsymbol{\Sigma}}$ can be derived from the Wishart distribution.

\begin{definition}[Wishart Distribution]
    Suppose $\mathbb{X}$ be a $p\times n$ matrix, each column of which is independently drawn from a $p$-variate normal distribution with zero mean:
    \begin{equation*}
        \mathbf{X}_{i}=\left(x_{i}^{1},\ldots,x_{i}^{p}\right)^{\prime}\sim N_{p}(0,\boldsymbol{\Sigma})
    \end{equation*}
    Then the Wishart distribution is the probability distribution of the $p\times p$ random matrix,
    \begin{equation}
        \mathbf{M}=\mathbb{X}^{\prime}\mathbb{X}=\sum_{i=1}^{n}\mathbf{X}_{i}\mathbf{X}_{i}^{\prime}
    \end{equation}
    and which can be denoted by
    \begin{equation*}
        \mathbf{M}\sim W_{p}\left(\boldsymbol{\Sigma},n\right)
    \end{equation*}
    If $p=\boldsymbol{\Sigma}=1$, then this distribution is a chi-squared distribution with $n$ degrees of freedom.
\end{definition}

\begin{note}
    If $n\geq p$, the probability density function of $\mathbf{M}$ is
    \begin{equation}
        f\left(\mathbf{M}\right)=\frac{1}{2^{np/2}\left[\operatorname{det}\left(\boldsymbol{\Sigma}\right)\right]^{n/2}\Gamma_{p}\left(\frac{n}{2}\right)}\operatorname{det}\left(\mathbf{M}\right)^{(n-p-1)/2}\exp\left[-\frac{1}{2}\operatorname{tr}\left(\boldsymbol{\Sigma}^{-1}\mathbf{M}\right)\right]
        \label{eq:pdf-wishart}
    \end{equation}
    with respect to Lebesque measure on the cone of symmetric positive definite matrices. Here, $\Gamma_{p}$ is the multivariate gamma function defined as
    \begin{equation*}
        \Gamma_{p}\left(\frac{n}{2}\right)=\pi^{p(p-1)/4}\prod_{j=1}^{p}\Gamma\left(\frac{n}{2}-\frac{j-1}{2}\right)
    \end{equation*}
\end{note}

The probability denisty function of $\widehat{\boldsymbol{\Sigma}}$ can be derived from (\ref{eq:pdf-wishart}), since
\begin{equation*}
    \mathbf{A}\sim W_{n}\left(\mathrm{I}_{n},m_{n}\right),\quad\operatorname{det}\left(\widehat{\boldsymbol{\Sigma}}\right)=m_{n}^{-n}\operatorname{det}\left(\mathbf{A}\right),\quad\operatorname{tr}\left(\widehat{\boldsymbol{\Sigma}}\right)=m_{n}^{-1}\operatorname{tr}\left(\mathbf{A}\right)
\end{equation*}
thus,
\begin{equation}
    \frac{m_{n}^{-n(m_{n}-n-1)/2+1}}{2^{m_{n}n/2}\Gamma_{n}\left(\frac{m_{n}}{2}\right)}\operatorname{det}\left(\widehat{\boldsymbol{\Sigma}}\right)^{(m_{n}-n-1)/2}\exp\left[-\frac{m_{n}}{2}\operatorname{tr}\left(\widehat{\boldsymbol{\Sigma}}\right)\right]
\end{equation}

\section{Laguerre Ensemble}

The joint probability density function of eigenvalues of $\widehat{\boldsymbol{\Sigma}}$ is
\begin{equation}
    p\left(\boldsymbol{\lambda}\right)=\widetilde{Q}_{m_{n}}^{-1}\exp\left(-\frac{m_{n}}{2}\sum_{k=1}^{n}\lambda_{k}\right)\prod_{k=1}^{n}\lambda_{k}^{(m_{n}-n-1)/2}\prod_{i<j}\left|\lambda_{i}-\lambda_{j}\right|
    \label{eq:jpdf-eigenvalues-sigma}
\end{equation}
where
\begin{equation*}
    0\leq\lambda_{1}\leq\ldots\leq\lambda_{n}<\infty
\end{equation*}
and $\widetilde{Q}_{m_{n}}$ is the normalization constant.

\begin{proof}
    Fisrt, we will give the characteristic function of $\widehat{\boldsymbol{\Sigma}}$, i.e.,
    \begin{equation*}
        \varphi_{\widehat{\boldsymbol{\Sigma}}}\left(\mathbf{P}\right)=E\left[\exp\left(i\sum_{1\leq i\leq j\leq n}P_{ij}\widehat{\boldsymbol{\Sigma}}_{ji}\right)\right]=E\left[\exp\left(i\operatorname{tr}\left(\mathbf{P}\widehat{\boldsymbol{\Sigma}}\right)\right)\right]
    \end{equation*}
    where $\left\{P_{ij}\right\}_{1\leq i\leq j\leq n}\in\mathbb{R}^{(n+1)n/2}$ and $\mathbf{P}$ is a real symmetric matrix, that
    \begin{equation*}
        \mathbf{P}=\left\{\widehat{P}_{ij},\widehat{P}_{ij}=\widehat{P}_{ji}\right\}_{i,j=1}^{n},\quad\widehat{P}_{ij}=\begin{cases}P_{ii}, & i=j \\ P_{ij} / 2, & i<j \end{cases}
    \end{equation*}
    Thus, we have
    \begin{equation*}
        \begin{aligned}
            = & \int_{\mathbb{R}^{m_{n}\times n}}\exp\left(i\operatorname{tr}\left(\mathbf{P}\widehat{\boldsymbol{\Sigma}}\right)\right)\cdot(2\pi)^{-m_{n}n/2}\exp\left(-\frac{1}{2}\sum_{k=1}^{m_{n}}\sum_{i=1}^{n}\left(X_{i}^{(k)}\right)^{2}\right)\prod_{k=1}^{m_{n}}\prod_{i=1}^{n}\,\mathrm{d}X_{i}^{(k)} \\
            = & \int_{\mathbb{R}^{m_{n}\times n}}(2\pi)^{-m_{n}n/2}\exp\left(-\frac{1}{2}\sum_{k=1}^{m_{n}}\sum_{i=1}^{n}\sum_{j=1}^{n}\mathbf{Q}_{ij}X_{i}^{(k)}X_{j}^{(k)}\right)\prod_{k=1}^{m_{n}}\prod_{i=1}^{n}\,\mathrm{d}X_{i}^{(k)}                                                                      \\
        \end{aligned}
    \end{equation*}
    where
    \begin{equation*}
        \mathbf{Q}=1-\frac{2i}{m_{n}}\mathbf{P}
    \end{equation*}

    Since $\left(X_{i}^{(k)}\right)_{1\leq i\leq n}$ are i.i.d. standard Gaussians,
    \begin{equation*}
        \begin{aligned}
            = & \left[\int_{\mathbb{R}^{n}}(2\pi)^{-n/2}\exp\left(-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\mathbf{Q}_{ij}X_{i}X_{j}\right)\prod_{i=1}^{n}\,\mathrm{d}X_{i}\right]^{m_{n}}                                                                                                                         \\
            = & \left[\int_{\mathbb{R}^{n}}(2\pi)^{-n/2}\exp\left(-\frac{1}{2}\mathbf{X}^{\prime}\mathbf{Q}\mathbf{X}\right)\,\mathrm{d}\mathbf{X}\right]^{m_{n}}                                                                                                                                                 \\
            = & \left[\operatorname{det}\left(\mathbf{Q}\right)^{-\frac{1}{2}}\int_{\mathbb{R}^{n}}(2\pi)^{-n/2}\exp\left(-\frac{1}{2}\left(\mathbf{Q}^{\frac{1}{2}}\mathbf{X}\right)^{\prime}\left(\mathbf{Q}^{\frac{1}{2}}\mathbf{X}\right)\right)\,\mathrm{d}\mathbf{Q}^{\frac{1}{2}}\mathbf{X}\right]^{m_{n}} \\
            = & \left[\operatorname{det}\left(\mathbf{Q}\right)\right]^{-m_{n}/2}
        \end{aligned}
    \end{equation*}
    thus,
    \begin{equation}
        \left[\operatorname{det}\left(\mathbf{Q}\right)\right]^{-m_{n}/2}=\left[\operatorname{det}\left(1-\frac{2i}{m_{n}}\mathbf{P}\right)\right]^{-m_{n}/2}=\prod_{k=1}^{n}\left(1-\frac{2i}{m_n}p_{k}\right)^{-m_{n}/2}
        \label{eq:characteristic-function-wishart-result-1}
    \end{equation}
    where $\{p_{k}\}_{k=1}^{n}$ are the eigenvalues of $\mathbf{P}$.

    Then, we will show that the characteristic function of (\ref{eq:jpdf-eigenvalues-sigma}) conincides with the above function. By the Wishart distribution, the probability denisty of the real symmetric and positive definite random matrix $\widehat{\boldsymbol{\Sigma}}$ is
    \begin{equation}
        \widetilde{Q}_{m_{n}}^{-1}\exp\left[-\frac{m_{n}}{2}\operatorname{tr}\left(\widehat{\boldsymbol{\Sigma}}\right)\right]\left[\operatorname{det}\left(\widehat{\boldsymbol{\Sigma}}\right)\right]^{(m_{n}-n-1)/2}\,\mathrm{d}\widehat{\boldsymbol{\Sigma}}
        \label{eq:wishart-distribution-sigma}
    \end{equation}
    where $\widetilde{Q}_{m_{n}}$ is the normalization constant. Then, the characteristic function of (\ref{eq:wishart-distribution-sigma}), i.e.,
    \begin{equation*}
        \widetilde{Q}_{m_{n}}^{-1}\int_{\mathcal{S}_{n}^{+}}\exp\left[i\operatorname{tr}\left(\mathbf{P}\widehat{\boldsymbol{\Sigma}}\right)-\frac{m_{n}}{2}\operatorname{tr}\left(\widehat{\boldsymbol{\Sigma}}\right)\right]\left[\operatorname{det}\left(\widehat{\boldsymbol{\Sigma}}\right)\right]^{(m_{n}-n-1)/2}\,\mathrm{d}\widehat{\boldsymbol{\Sigma}}
    \end{equation*}
    where the integration is over the set $\mathcal{S}_{n}^{+}$ of $n\times n$ real symmetric and positive definite matrices.

    Since
    \begin{equation*}
        \sum_{k=1}^{n}\lambda_{k}=\operatorname{tr}\left(\widehat{\boldsymbol{\Sigma}}\right),\quad\prod_{k=1}^{n}\lambda_{k}^{(m_{n}-n-1)/2}=\left[\operatorname{det}\left(\widehat{\boldsymbol{\Sigma}}\right)\right]^{(m_{n}-n-1)/2}
    \end{equation*}
    and
    \begin{equation*}
        \mathrm{d}\widehat{\boldsymbol{\Sigma}}=\prod_{i<j}\left|\lambda_{i}-\lambda_{j}\right|\,\mathrm{d}\boldsymbol{\lambda}H_{1}\left(\mathrm{d}O\right)
    \end{equation*}
    where $H_{1}$ is the normalized Haar measure of $O(n)$, and the integration over $\boldsymbol{\lambda}$ and $O\in O(n)$ are independent.

    Since the orthogonal invariance of the density of (\ref{eq:wishart-distribution-sigma}), we obtain (\ref{eq:jpdf-eigenvalues-sigma}), and the characteristic function is
    \begin{equation}
        Q_{n}^{-1}\int_{\left(\mathbb{R}_{+}\right)^{n}}\exp\left[\sum_{k=1}^{n}\left(i p_{k}-\frac{m_{n}}{2}\right)\lambda_{k}\right]\prod_{k=1}^{n}\lambda_{k}^{(m_{n}-n-1)/2}\prod_{i<j}\left|\lambda_{i}-\lambda_{j}\right|\,\mathrm{d}\boldsymbol{\lambda}
        \label{eq:characteristic-function-wishart}
    \end{equation}
    where $Q_{m_{n}}=m_{n}!\widetilde{Q}_{m_{n}}$.

    If we viewed (\ref{eq:characteristic-function-wishart-result-1}) and (\ref{eq:characteristic-function-wishart}) as the function of $\{p_{k}\}_{k=1}^{n}\in\mathbb{R}^{n}$, then they can be \textbf{analytic continuation} to the domain
    \begin{equation*}
        \left\{p_{k}+i p_{k}^{\prime},p_{k}^{\prime}\geq 0\right\}_{k=1}^{n}
    \end{equation*}

    If we replace $\left\{p_{k}\right\}_{k=1}^{n}$ by $\left\{ip_{k}^{\prime},p_{k}^{\prime}\geq 0\right\}_{k=1}^{n}$ on (\ref{eq:characteristic-function-wishart-result-1}), since this is a set of uniqueness of both (\ref{eq:characteristic-function-wishart-result-1}) and (\ref{eq:characteristic-function-wishart}) analytic functions, we have
    \begin{equation*}
        Q_{m_{n}}^{-1}\int_{\left(\mathbb{R}_{+}\right)^{n}}\exp\left[-\frac{m_{n}}{2}\sum_{k=1}^{n}q_{k}\lambda_{k}\right]\prod_{k=1}^{n}\lambda_{k}^{(m_{n}-n-1)/2}\prod_{i<j}\left|\lambda_{i}-\lambda_{j}\right|\,\mathrm{d}\boldsymbol{\lambda}
    \end{equation*}
    where $q_{k}=1+\frac{2p_{k}^{\prime}}{m_{n}}\geq 1,k=1,\ldots,n$, and since
    \begin{equation*}
        \forall i,j\quad\frac{q_{i}}{q_{j}}=\frac{1+\frac{2p_{i}^{\prime}}{m_{n}}}{1+\frac{2p_{j}^{\prime}}{m_{n}}}\rightarrow 1,\quad\text{ as }\quad m_{n}\rightarrow\infty
    \end{equation*}
    we have
    \begin{equation*}
        \prod_{i<j}\left|q_{i}\lambda_{i}-q_{j}\lambda_{j}\right|=\prod_{i<j}q_{i}\left|\lambda_{i}-\frac{q_{j}}{q_{i}}\lambda_{j}\right|\rightarrow\prod_{k=1}^{n}q_{k}^{(n-1)/2}\prod_{i<j}\left|\lambda_{i}-\lambda_{j}\right|,\quad\text{ as }\quad m_{n}\rightarrow\infty
    \end{equation*}
    thus,
    \begin{equation*}
        \begin{array}{c}
            \prod_{k=1}^{n}q_{k}^{-m_{n}/2}\cdot Q_{m_{n}}^{-1}\int_{\left(\mathbb{R}_{+}\right)^{n}}\exp\left[-\frac{m_{n}}{2}\sum_{k=1}^{n}q_{k}\lambda_{k}\right]\prod_{k=1}^{n}\left(q_{k}\lambda_{k}\right)^{(m_{n}-n-1)/2}\cdot \\
            \prod_{i<j}\left|q_{i}\lambda_{i}-q_{j}\lambda_{j}\right|\,\mathrm{d}\mathbf{q}\boldsymbol{\lambda}
        \end{array}
    \end{equation*}
    Since
    \begin{equation*}
        \forall k\quad q_{k}\lambda_{k}\rightarrow\lambda_{k},\quad\text{ as }\quad m_{n}\rightarrow\infty
    \end{equation*}
    we can "lifting" from $\left\{\lambda_{k}\right\}_{k=1}^{n}$ to $\mathcal{S}_{n}^{+}$ bring the integral to
    \begin{equation*}
        \prod_{k=1}^{n}\left(1+\frac{2p_{k}^{\prime}}{m_{n}}\right)^{-m_{n}/2}\widetilde{Q}_{n}^{-1}\int_{\mathcal{S}_{n}^{+}}\exp\left[-\frac{m_{n}}{2}\operatorname{tr}\left(\widehat{\boldsymbol{\Sigma}}\right)\right]\left[\operatorname{det}\left(\widehat{\boldsymbol{\Sigma}}\right)\right]^{(m_{n}-n-1)/2}\,\mathrm{d}\widehat{\boldsymbol{\Sigma}}
    \end{equation*}
    The integral here is equal to $\widetilde{Q}_{n}$, the normalization constant of the probability measure (\ref{eq:wishart-distribution-sigma}).

    If we replace $\left\{i p_{k}^{\prime}\right\}_{k=1}^{n}$ back by $\left\{p_{k}\right\}_{k=1}^{n}$, then the above expression is
    \begin{equation*}
        \prod_{k=1}^{n}\left(1-\frac{2i}{m_n}p_{k}\right)^{-m_{n}/2}
    \end{equation*}
    which coincides with (\ref{eq:characteristic-function-wishart-result-1}).

    Thus the probability law of the Wishart matrices of $\boldsymbol{\Sigma}$ given by (\ref{eq:wishart-distribution-sigma}) implies that the corresponding joint probability density of eigenvalues is given by (\ref{eq:jpdf-eigenvalues-sigma}) for $\boldsymbol{\Sigma}$.
\end{proof}

\begin{definition}[Laguerre Orthogonal Ensemble]
    For the $n\times n$ Laguerre orthogonal ensembles\footnote{Ensemble is often used in physics and the physics-influenced literature. In probability theory, the term \textbf{probability space} is more prevalent.} of statistics, the joint probability density function of eigenvalues is
    for arbitrary parameter $\beta>0$ and $\alpha>-\frac{2}{\beta}$, is
    \begin{equation}
        p\left(\boldsymbol{\lambda}\right)=K_{\alpha,\beta}\exp\left(-\frac{\beta}{2}\sum_{k=1}^{n}\lambda_{k}\right)\prod_{k=1}^{n} \lambda_{k}^{\frac{\alpha\beta}{2}}\prod_{i<j}\left|\lambda_{i}-\lambda_{j}\right|^{\beta}
        \label{eq:laguerre-orthogonal-ensemble}
    \end{equation}
    where
    \begin{equation*}
        0\leq\lambda_{1}\leq\ldots\leq\lambda_{n}<\infty
    \end{equation*}
    and $K_{n,m}$ are normalization constant.
\end{definition}

And Equation (\ref{eq:laguerre-orthogonal-ensemble}) can be written in the standard Boltzmann-Gibbs form, that,
\begin{equation*}
    p\left(\boldsymbol{\lambda}\right)\propto\exp\left[-\beta E\left(\boldsymbol{\lambda}\right)\right]
\end{equation*}
where
\begin{equation}
    E\left(\boldsymbol{\lambda}\right)=\frac{1}{2}\sum_{k=1}^{n}\left(\lambda_{k}-\alpha\log\lambda_{k}\right)-\frac{1}{2}\sum_{i\neq j}\left|\lambda_{i}-\lambda_{j}\right|
\end{equation}

For the (\ref{eq:jpdf-eigenvalues-sigma}), which can be written as (\ref{eq:laguerre-orthogonal-ensemble}) form, that,
\begin{equation*}
    p\left(\boldsymbol{\lambda}\right)\propto\exp\left[-\beta m_{n}E\left(\boldsymbol{\lambda}\right)\right]
\end{equation*}
where $\beta=1$ and
\begin{equation*}
    E\left(\boldsymbol{\lambda}\right)=\frac{m_{n}}{2}\sum_{k=1}^{n}\left[\lambda_{k}-\left(\frac{m_{n}-n-1}{m_{n}}\right)\log\lambda_{k}\right]-\frac{1}{2m_{n}}\sum_{i\neq j}\left|\lambda_{i}-\lambda_{j}\right|
\end{equation*}

\section{Marchenko-Pastur Theorem}

In this section, we will invastiage the empirical spectral measure of $\boldsymbol{\Sigma}$, which converges to a nonrandom distribution --- Marchenko-Pastur distribution. Before further proof, we will introduce some basic concepts and tools.

\begin{definition}[Empirical Spectral Measure]
    For a symmetric matrix $\mathbf{M}\in\mathbb{R}^{n\times n}$, the spectral measure or empirical spectral measure or empirical spectral distribution (e.s.d.) $\mu_{\mathbf{M}}$ of $\mathbf{M}$ is defined as the normalized counting measure of the eigenvalues $\lambda_{1}(\mathbf{M}),\ldots,\lambda_{n}(\mathbf{M})$ of $\mathbf{M}$, i.e.,
    \begin{equation}
        \mu_{\mathbf{M}}:=\frac{1}{n}\sum_{i=1}^{n}\delta_{\lambda_{i}(\mathbf{M})}
    \end{equation}
\end{definition}

\begin{remark}
    Since $\int\mu_{\mathbf{M}}\left(\mathrm{d}x\right)=1$, the spectral measure $\mu_{\mathbf{M}}$ of a matrix $\mathbf{M}\in\mathbb{R}^{n\times n}$ (random or not) is a probability measure.
\end{remark}

\begin{definition}[Resolvent]
    For a symmetric matrix $\mathbf{M}\in\mathbb{R}^{n\times n}$, the resolvent $\mathbf{Q}_{\mathbf{M}}(z)$ of $\mathbf{M}$ is defined as
    \begin{equation}
        \mathbf{Q}_{\mathbf{M}}(z):=\left(\mathbf{M}-z\mathbf{I}_{n}\right)^{-1}
    \end{equation}
    where $z\in\mathbb{C}$ not eigenvalue of $\mathbf{M}$.
\end{definition}

\begin{definition}[Stieltjes Transform]
    For a real probability measure $\mu$ with support $\operatorname{supp}(\mu)$, the Stieltjes transform $m_{\mu}(z)$ is defined as
    \begin{equation}
        m_{\mu}(z):=\int\frac{1}{t-z}\mu\left(\mathrm{d}t\right)
    \end{equation}
    where $z\in\mathbb{C}\backslash\operatorname{supp}(\mu)$.
\end{definition}

As a transform, $m_{\mu}$ has an inverse formula to recover $\mu$, as per the following result.

\begin{theorem}[Inverse Stieltjes Transform]
    For $a,b$ continuity points of the probability measure $\mu$, we have
    \begin{equation}
        \mu\left([a,b]\right)=\frac{1}{\pi}\lim_{y\downarrow 0}\int_{a}^{b}\operatorname{Im}\left[m_{\mu}(x+iy)\right]\,\mathrm{d}x
    \end{equation}
    Specially, if $\mu$ has a density $f$ at $x$, then
    \begin{equation}
        f(x)=\frac{1}{\pi}\lim_{y\downarrow 0}\operatorname{Im}\left[m_{\mu}(x+iy)\right]
    \end{equation}
    And, if $\mu$ has an isolated mass at $x$, then
    \begin{equation}
        \mu(\{x\})=\lim_{y \downarrow 0}-iy m_{\mu}(x+iy)
    \end{equation}
\end{theorem}

\begin{proof}
    Since
    \begin{equation*}
        \left|\frac{y}{(t-x)^{2}+y^{2}}\right|\leq\frac{1}{y},\quad\forall y>0
    \end{equation*}
    by Fubini's theorem,
    \begin{equation*}
        \begin{aligned}
            \frac{1}{\pi}\int_{a}^{b}\operatorname{Im}\left[m_{\mu}(x+iy)\right]\,\mathrm{d}x= & \frac{1}{\pi}\int_{a}^{b}\left[\int\frac{y}{(t-x)^{2}+y^{2}}\mu(\mathrm{d}t)\right]\,\mathrm{d}x              \\
            =                                                                                  & \frac{1}{\pi}\int\left[\int_{a}^{b}\frac{y}{(t-x)^{2}+y^{2}}\,\mathrm{d}x\right]\mu(\mathrm{d}t)              \\
            =                                                                                  & \frac{1}{\pi} \int\left[\arctan \left(\frac{b-t}{y}\right)-\arctan \left(\frac{a-t}{y}\right)\right] \mu(d t)
        \end{aligned}
    \end{equation*}

    As $y\downarrow 0$, the difference in brackets converges either to $\pm \pi$ or 0 depending on the relative position of $a, b$ and $t$.

    By the dominated convergence theorem, the limit, as $y \downarrow 0$, is $\int 1_{[a, b]} \mu(d t)=\mu([a, b])$.

    When $\mu$ has an isolated mass at $x$, say $\mu(d t)=a \delta_{x}(t)$, we similarly have, again by dominated convergence (using in particular $\left.|y(t-x)| \leq \frac{1}{2}\left(y^{2}+(t-x)^{2}\right)\right)$,
    $$
        \lim _{y \downarrow 0}-iy m(x+iy)=-\lim _{y \downarrow 0} \int \frac{iy(t-x) \mu(d t)}{(t-x)^{2}+y^{2}}+\lim _{y \downarrow 0} \int \frac{y^{2} \mu(d t)}{(t-x)^{2}+y^{2}}=a .
    $$
\end{proof}

With the above tools, we can prove the Marchenko-Pastur Theorem.
\begin{theorem}[Marchenko-Pastur Theorem]
    If
    \begin{equation*}
        \frac{n}{m_{n}}\rightarrow\rho\in(0,\infty),\quad\text{ as }\quad n\rightarrow\infty
    \end{equation*}
    and the empirical spectral measure of $\widehat{\boldsymbol{\Sigma}}$ tends weakly with probability 1 to the nonrandom measure, i.e.,
    \begin{equation}
        \frac{1}{m_{n}}\sum_{k=1}^{n}\delta_{\lambda_{k}(\widehat{\boldsymbol{\Sigma}})}\stackrel{d}{\rightarrow}\mu_{\rho},\quad\text{ a.s. }
    \end{equation}
    where $\mu_{\rho}$ is the Marchenko-Pastur distribution on $\left[a^{-},a^{+}\right]$ with $a^{\pm}=(1\pm\sqrt{\rho})^{2}$ given by
    \begin{equation}
        \mu_{\rho}(\mathrm{d}x)=\frac{\rho-1}{\rho}\mathbf{I}_{\rho>1}\delta_{0}+\frac{\sqrt{\left(a^{+}-x\right)\left(x-a^{-}\right)}}{\rho 2\pi x}\mathbf{I}_{\left[a^{-},a^{+}\right]}(x)\,\mathrm{d}x
    \end{equation}
\end{theorem}

\begin{proof}
    The normalization constant of eigenvalues of $\widehat{\boldsymbol{\Sigma}}$ is
    \begin{equation*}
        \begin{aligned}
            \widetilde{Q}_{m_{n}}= & \int_{\left(\mathbb{R}_{+}\right)^{n}}\exp\left(-\frac{m_{n}}{2}\sum_{k=1}^{n}\lambda_{k}\right)\prod_{k=1}^{n}\lambda_{k}^{(m_{n}-n-1)/2}\prod_{i<j}\left|\lambda_{i}-\lambda_{j}\right|\,\mathrm{d}\boldsymbol{\lambda} \\
            =                      & \int_{\left(\mathbb{R}_{+}\right)^{n}}\exp\left[-m_{n}E\left(\boldsymbol{\lambda}\right)\right]\,\mathrm{d}\boldsymbol{\lambda}
        \end{aligned}
    \end{equation*}
    where
    \begin{equation*}
        E\left(\boldsymbol{\lambda}\right)=\frac{m_{n}}{2}\sum_{k=1}^{n}\left[\lambda_{k}-\left(\frac{m_{n}-n-1}{m_{n}}\right)\log\lambda_{k}\right]-\frac{1}{2m_{n}}\sum_{i\neq j}\left|\lambda_{i}-\lambda_{j}\right|
    \end{equation*}
\end{proof}
