\chapter{Unconstrained Minimization}

\section{Definition of Unconstrained Minimization}

\begin{definition}[Unconstrained Minimization Problem]
    The unconstrained minimization problem is defined to be
    \begin{equation}
        \min_{\mathbf{x}}f(\mathbf{x})
        \label{eq:unconstrained-minimization-problem}
    \end{equation}
    where $f:\mathbf{R}^n\rightarrow\mathbf{R}$ is convex and twice continuously differentiable.
\end{definition}

Assume the problem is solvable, i.e., there exists an optimal point $\mathbf{x}^{*}$, such that,
\begin{equation*}
    f(\mathbf{x}^{*})=\inf_{\mathbf{x}}f(\mathbf{x})
\end{equation*}
and denote it by $p^{*}$. Since $f$ is differentiable and convex, the point $\mathbf{x}^{*}$ be the optimal. if and only if
\begin{equation}
    \nabla f(\mathbf{x}^{*})=0
    \label{eq:optimal-condition-of-unconstrained-minimization-problem}
\end{equation}

Solving (\ref{eq:unconstrained-minimization-problem}) is equal to finding the solution of (\ref{eq:optimal-condition-of-unconstrained-minimization-problem}), thus (\ref{eq:unconstrained-minimization-problem}) can be solved by analytic solution of (\ref{eq:optimal-condition-of-unconstrained-minimization-problem}) in a few cases, but usually can be solved by an iterative algorithm, i.e.,
\begin{equation*}
    \exists\mathbf{x}^{(0)},\mathbf{x}^{(1)},\ldots\in\operatorname{dom}f,\quad\text{ s.t. }f(\mathbf{x}^{(k)})\rightarrow p^{*},\quad\text{ as }\quad k\rightarrow\infty
\end{equation*}
where the initial point $\mathbf{x}^{(0)}$ must lie in $\operatorname{dom}f$, and the sublevel set $$S=\left\{\mathbf{x}\in\operatorname{dom}f\mid f(\mathbf{x})\leq f(\mathbf{x}^{0})\right\}$$ must be closed.

\begin{remark}
\end{remark}

\begin{example}[Quadratic Minimization]
    The general convex quadratic minimization problem has the form
    \begin{equation}
        \min_{\mathbf{x}}\,\frac{1}{2}\mathbf{x}^{\prime}\mathbf{P}\mathbf{x}+\mathbf{q}^{\prime}\mathbf{x}+r \label{eq:quadratic-minimization}
    \end{equation}
    where $\mathbf{P}\in\mathbb{S}_{+}^{n},\mathbf{q}\in\mathbb{R}^{n}$, and $r\in\mathbb{R}$.

    The optimality condition is
    \begin{equation}
        \mathbf{P}\mathbf{x}^{*}+\mathbf{q}=\mathbf{0}
        \label{eq:quadratic-minimization-optimality-condition}
    \end{equation}
    which is a set of linear equations.
    \begin{enumerate}
        \item If $\mathbf{P}\succ 0$, exists a unique solution $\mathbf{x}^{*}=-\mathbf{P}^{-1}\mathbf{q}$.

        \item If $\mathbf{P}$ is not positive definite, any solution of (\ref{eq:quadratic-minimization-optimality-condition}) is optimal for (\ref{eq:quadratic-minimization}).

              Since $\mathbf{P}\nsucceq 0$, i.e.,
              \begin{equation*}
                  \exists\mathbf{v},\quad\text{ s.t. }\mathbf{v}^{\prime}\mathbf{P}\mathbf{v}<0
              \end{equation*}
              Let $\mathbf{x}=t\mathbf{v}$, we have
              \begin{equation*}
                  f\left(\mathbf{x}\right)=t^{2}\left(\mathbf{v}^{\prime}\mathbf{P}\mathbf{v}/2\right)+t\left(\mathbf{q}^{\prime}\mathbf{v}\right)+r
              \end{equation*}
              which converges to $-\infty$ as $t\rightarrow\infty$.

        \item If (\ref{eq:quadratic-minimization-optimality-condition}) does not have a solution, then (\ref{eq:quadratic-minimization}) is unbounded.

              Since (\ref{eq:quadratic-minimization-optimality-condition}) does not have a solution, i.e.,
              \begin{equation*}
                  \mathbf{q}\notin\mathcal{R}(\mathbf{P})
              \end{equation*}
              Let
              \begin{equation*}
                  \mathbf{q}=\tilde{\mathbf{q}}+\mathbf{v}
              \end{equation*}
              where $\tilde{\mathbf{q}}$ is the Euclidean projection of $\mathbf{q}$ onto $\mathcal{R}(\mathbf{P})$, and $\mathbf{v}=\mathbf{q}-\tilde{\mathbf{q}}$. And $\mathbf{v}$ is nonzero and orthogonal to $\mathcal{R}(\mathbf{P})$, i.e., $\mathbf{v}^{\prime}\mathbf{P}\mathbf{v}=0$. If we take $\mathbf{x}=t\mathbf{v}$, we have
              \begin{equation*}
                  f(\mathbf{x})=t\mathbf{q}^{\prime}\mathbf{v}+r=t(\tilde{\mathbf{q}}+\mathbf{v})^{\prime}\mathbf{v}+r=t(\mathbf{v}^{\prime}\mathbf{v})+r
              \end{equation*}
              which is unbounded below.
    \end{enumerate}
\end{example}

\begin{remark}
    The least-squares problem is a special case of quadratic minimization, that,
    \begin{equation}
        \min_{\mathbf{x}}\,\|\mathbf{A}\mathbf{x}-\mathbf{b}\|_{2}^{2}=\mathbf{x}^{\prime}\left(\mathbf{A}^{\prime}\mathbf{A}\right)\mathbf{x}-2\left(\mathbf{A}^{\prime}\mathbf{b}\right)^{\prime}\mathbf{x}+\mathbf{b}^{\prime}\mathbf{b}
    \end{equation}
    The optimality condition is
    \begin{equation}
        \mathbf{A}^{\prime}\mathbf{A}\mathbf{x}^{*}=\mathbf{A}^{\prime}\mathbf{b}
    \end{equation}
    are called the normal equations of the least-squares problem.
\end{remark}

\begin{example}[Unconstrained Geometric Programming]
    The unconstrained geometric program in convex form
    \begin{equation}
        \min_{\mathbf{x}}\,f(\mathbf{x})=\log \left(\sum_{i=1}^{m}\exp\left(\mathbf{a}_{i}^{\prime}\mathbf{x}+b_{i}\right)\right)
    \end{equation}
    The optimality condition is
    \begin{equation}
        \nabla f\left(x^{*}\right)=\frac{\sum_{i=1}^{m}\exp\left(\mathbf{a}_{i}^{\prime}\mathbf{x}^{*}+b_{i}\right)\mathbf{a}_{i}}{\sum_{j=1}^{m}\exp\left(\mathbf{a}_{j}^{\prime}\mathbf{x}^{*}+b_{j}\right)}=\mathbf{0}
    \end{equation}
    which has no analytical solution, so we must resort to an iterative algorithm. For this problem, $\operatorname{dom} f=\mathbb{R}^{n}$, so any point can be chosen as the initial point $\mathbf{x}^{(0)}$.
\end{example}

\begin{example}[Analytic Center of Linear Inequalities]

\end{example}

\begin{definition}[Strong Convexity]
    
\end{definition}

\section{General Descent Method}

\section{Gradient Descent Method}

\section{Steepest Descent Method}

\section{Newton's Method}