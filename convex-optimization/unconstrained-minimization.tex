\chapter{Unconstrained Minimization}

\section{Definition of Unconstrained Minimization}

\begin{definition}[Unconstrained Minimization Problem]
    The unconstrained minimization problem is defined to be
    \begin{equation}
        \min_{\mathbf{x}}f(\mathbf{x})
        \label{eq:unconstrained-minimization-problem}
    \end{equation}
    where $f:\mathbf{R}^n\rightarrow\mathbf{R}$ is convex and twice continuously differentiable.
\end{definition}

Assume the problem is solvable, i.e., there exists an optimal point $\mathbf{x}^{*}$, such that,
\begin{equation*}
    f(\mathbf{x}^{*})=\inf_{\mathbf{x}}f(\mathbf{x})
\end{equation*}
and denote it by $p^{*}$. Since $f$ is differentiable and convex, the point $\mathbf{x}^{*}$ be the optimal. if and only if
\begin{equation}
    \nabla f(\mathbf{x}^{*})=0
    \label{eq:optimal-condition-of-unconstrained-minimization-problem}
\end{equation}

Solving (\ref{eq:unconstrained-minimization-problem}) is equal to finding the solution of (\ref{eq:optimal-condition-of-unconstrained-minimization-problem}), thus (\ref{eq:unconstrained-minimization-problem}) can be solved by analytic solution of (\ref{eq:optimal-condition-of-unconstrained-minimization-problem}) in a few cases, but usually can be solved by an iterative algorithm, i.e.,
\begin{equation*}
    \exists\mathbf{x}^{(0)},\mathbf{x}^{(1)},\ldots\in\operatorname{dom}f,\quad\text{ s.t. }f\left(\mathbf{x}^{(k)}\right)\rightarrow p^{*},\quad\text{ as }\quad k\rightarrow\infty
\end{equation*}

\begin{example}[ Quadratic Minimization]
    The general convex quadratic minimization problem has the form
    \begin{equation}
        \min_{\mathbf{x}}\,\frac{1}{2}\mathbf{x}^{\prime}\mathbf{P}\mathbf{x}+\mathbf{q}^{\prime}\mathbf{x}+r \label{eq:quadratic-minimization}
    \end{equation}
    where $\mathbf{P}\in\mathbb{S}_{+}^{n},\mathbf{q}\in\mathbb{R}^{n}$, and $r\in\mathbb{R}$.
\end{example}

\begin{solution}
    The optimality conditions is
    \begin{equation*}
        \mathbf{P}\mathbf{x}^{*}+\mathbf{q}=\mathbf{0}
    \end{equation*}
    which is a set of linear equations.
    \begin{enumerate}
        \item If $\mathbf{P}\succ 0$, exists aunique solution $\mathbf{x}^{*}=-\mathbf{P}^{-1}\mathbf{q}$.

        \item If $\mathbf{P}$ is not positive definite, any solution of $\mathbf{P}\mathbf{x}^{*}=-\mathbf{q}$ is optimal for (\ref{eq:quadratic-minimization}).

              Since $\mathbf{P}$ is not positive definite, i.e., $\mathbf{P}\nsucceq 0$,
              \begin{equation*}
                  \exists\mathbf{v},\quad\text{ s.t. }\mathbf{v}^{\prime}\mathbf{P}\mathbf{v}<0
              \end{equation*}
              Let $\mathbf{x}=t\mathbf{v}$, we have
              \begin{equation*}
                  f\left(\mathbf{x}\right)=t^{2}\left(\mathbf{v}^{\prime}\mathbf{P}\mathbf{v}/2\right)+t\left(\mathbf{q}^{\prime}\mathbf{v}\right)+r
              \end{equation*}
              which converges to $-\infty$ as $t\rightarrow\infty$.

        \item If $\mathbf{P}\mathbf{x}^{*}=-\mathbf{q}$ does not have a solution, then (\ref{eq:quadratic-minimization}) is unbounded.

              This means $q \notin \mathcal{R}(P)$. Express $q$ as $q=\tilde{q}+v$, where $\tilde{q}$ is the Euclidean projection of $q$ onto $\mathcal{R}(P)$, and take $v=q-\tilde{q}$. This vector is nonzero and orthogonal to $\mathcal{R}(P)$,
              i.e., $v^{T} P v=0$. It follows that for $x=t v$, we have
              $$
                  f(x)=t q^{T} v+r=t(\tilde{q}+v)^{T} v+r=t\left(v^{T} v\right)+r
              $$
              which is unbounded below.
    \end{enumerate}
\end{solution}

\begin{example}[ Least Square Estimation]

\end{example}

\begin{example}[ Unconstrained Geometric Programming]

\end{example}

\begin{example}[ Analytic Center of Linear Inequalities]

\end{example}

\section{General Descent Method}

\section{Gradient Descent Method}

\section{Steepest Descent Method}

\section{Newton's Method}