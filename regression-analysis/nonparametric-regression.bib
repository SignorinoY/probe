@inproceedings{bousquet2001algorithmic,
  title     = {Algorithmic Stability and Generalization Performance},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author    = {Bousquet, Olivier and Elisseeff, André},
  date      = {2001},
  volume    = {13},
  publisher = {MIT Press},
  url       = {https://papers.nips.cc/paper/2000/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html},
  urldate   = {2022-01-08},
  langid    = {english},
  keywords  = {Jab/Pre,linter/error},
}

@article{bousquet2002stability,
  title        = {Stability and Generalization},
  author       = {Bousquet, Olivier and Elisseeff, André},
  date         = {2002},
  journaltitle = {Journal of Machine Learning Research},
  shortjournal = {J. Mach. Learn. Res.},
  volume       = {2},
  pages        = {499--526},
  issn         = {ISSN 1533-7928},
  url          = {https://www.jmlr.org/papers/v2/bousquet02a.html},
  urldate      = {2022-01-08},
  abstract     = {We define notions of stability for learning algorithms and show how to use these notions to derive generalization error bounds based on the empirical error and the leave-one-out error. The methods we use can be applied in the regression framework as well as in the classification one when the classifier is obtained by thresholding a real-valued function. We study the stability properties of large classes of learning algorithms such as regularization based algorithms. In particular we focus on Hilbert space regularization and Kullback-Leibler regularization. We demonstrate how to apply the results to SVM for regression and classification.},
  issue        = {Mar},
  langid       = {english},
}

@article{hofmann2008kernel,
  title        = {Kernel Methods in Machine Learning},
  author       = {Hofmann, Thomas and Schölkopf, Bernhard and Smola, Alexander J.},
  date         = {2008-06-01},
  journaltitle = {The Annals of Statistics},
  shortjournal = {Ann. Statist.},
  volume       = {36},
  number       = {3},
  issn         = {0090-5364},
  doi          = {10.1214/009053607000000677},
  langid       = {english},
  annotation   = {TLDR: A review of machine learning methods employing positive definite kernels, ranging from binary classifiers to sophisticated methods for estimation with structured data, which include nonlinear functions as well as functions defined on nonvectorial data.},
}

@incollection{mcdiarmid1989method,
  title     = {On the Method of Bounded Differences},
  booktitle = {Surveys in {{Combinatorics}}, 1989},
  author    = {McDiarmid, Colin},
  editor    = {Siemons, J.},
  date      = {1989-08-03},
  series    = {London {{Mathematical Society Lecture Note Series}}},
  edition   = {1},
  pages     = {148--188},
  publisher = {Cambridge University Press},
  location  = {Cambridge},
  doi       = {10.1017/CBO9781107359949.008},
  isbn      = {978-0-521-37823-9 978-1-107-35994-9},
  langid    = {english},
}


@unpublished{ofer2011algorithmic,
  title  = {Algorithmic Stability},
  author = {{Ofer Dekel} and {Thach Nguyen}},
  date   = {2011-08-03},
  langid = {english},
}


@book{scholkopf2002learning,
  title      = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},
  shorttitle = {Learning with Kernels},
  author     = {Schölkopf, Bernhard},
  namea      = {Smola, Alexander J.},
  nameatype  = {collaborator},
  date       = {2002},
  series     = {Adaptive Computation and Machine Learning},
  publisher  = {MIT Press},
  location   = {Cambridge, Mass},
  abstract   = {In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs -- -kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics. Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years},
  isbn       = {978-0-262-19475-4 978-0-262-25693-3 978-0-585-47759-6},
  langid     = {english},
}



@unpublished{stephane2014lecture,
  title  = {Lecture 4: Kernels and Associated Functions},
  author = {{Stéphane Canu}},
  date   = {2014-03-04},
  url    = {https://cel.archives-ouvertes.fr/cel-01003007/file/Lecture4_Kenrels_Functions_RKHS.pdf},
  langid = {english},
  venue  = {Sao Paulo},
}
