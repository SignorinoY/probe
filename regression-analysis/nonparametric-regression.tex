\chapter{Nonparametric Regression}

\section{Uniform Stability of Regularized Kernel Model}

\subsection{Introduction}

Suppose $\mathcal{X}$ be the input space, $\mathcal{Y}$ be the output space, $\mathcal{D}$ be some (almost) completely unknown probability distribution on $\mathcal{X}\times\mathcal{Y}$. Given the $n$ i.i.d observed data, which sampled from an unknown distribution $\mathcal{D}$, that,
\begin{equation}
	S:=\left\{(\mathbf{x}_{1},y_{1}),\ldots,(\mathbf{x}_{n},y_{n})\right\}\subseteq\mathcal{D}
\end{equation}
and the goal of us is to estimate the functional relationship between $\mathcal{X}$ and $\mathcal{Y}$.

To formalize the problem, we now aim at finding a predictor fucntion $f^{*}$ among the function space $\mathcal{F}:=\left\{f:\mathcal{X}\rightarrow\mathcal{Y}\right\}$ based on the observed data $S$, which minimizes the true risk
\begin{equation} \label{eq:true-risk}
	R[f]:=\mathbb{E}_{\mathcal{D}}\left[L\left(y,f(\mathbf{x})\right)\right]
\end{equation}
where $L:\mathcal{Y}\times\mathcal{Y}\rightarrow\mathbb{R}$ is an arbitrary convex loss function, typically assumed that the smaller $L\left(y,f(\mathbf{x})\right)$ is, the better the approximmation of $y$ is. Thus, we are trying to find a predictor $f^{*}$ with risk close to the optimal risk
\begin{equation}
	R^{*}:=\inf\left\{R[f]\mid f:\mathcal{X}\rightarrow\mathcal{Y}\right\}
\end{equation}
% However, for many practical algorithms, it is often observed that the true risk \eqref{eq:true-risk} is not too far from the empirical error.

Finding the predictor function $f^{*}$ which minimizing the empirical risk
\begin{equation}
	R_{\text{emp}}(f)=\frac{1}{n}\sum_{i=1}^{n}L\left(y_{i},f(\textbf{x}_{i})\right)
\end{equation}
is a natural things for us to be trying to do. However, as it known to all, just minimizing the empirical risk is suicidal, which almost certainly leads to overfitting. Minimizing $R_{\text{emp}}$ only makes sense if we simultaneously somehow restrict ourselves to the $\mathcal{F}$, that are of just the right level of complexity. One way to do this is by explicitly restricting the function space $\mathcal{F}$ to a "simple" space, as in structural risk minimization, which is to introduce a penalty functional $\Omega[f]$ that somehow measures the complexity of each function $f\in\mathcal{F}$, and to minimize the regularized risk
\begin{equation}
	R_{\text{reg}}[f]=R_{\text{emp}}[f]+\Omega[f]
\end{equation}

In this report, we restrict the predictor function $f\in\mathcal{F}$ among the reproducing kernel Hilbert space $\mathcal{H}$, and the regularized risk has the form
\begin{equation}
	\label{eq:regularized-risk-of-rkhs}
	R_{\text{reg}}[f]=\frac{1}{n}\sum_{i=1}^{n}L\left(y_{i},f(\mathbf{x}_{i})\right)+\frac{\lambda}{2}\|f\|_{\mathcal{H}}^{2}
\end{equation}
thus, we can estimate $f^{*}$ by soving the following optimization problem
\begin{equation}
	\hat{f}=\underset{f\in\mathcal{H}}{\arg\min}\,\frac{1}{n}\sum_{i=1}^{n}L\left(y_{i},f(\mathbf{x}_{i})\right)+\frac{\lambda}{2}\|f\|_{\mathcal{H}}^{2}
\end{equation}
where $\lambda>0$ is a regularization parameter to reduce the danger of overfitting. Since $L\left(y,f(\mathbf{x})\right)$ is convex in $f$, the minimizer $\hat{f}$ is uniquely determined and a simple gradient descent algorithm can be used to find $\hat{f}$. So the main focus of this reports is to answer a remain question we want to know, whether the risk $R[\hat{f}]$ is close to the optimal risk $R^{*}$, which will influence the stability of our algorithm.

\subsection{Some Notations and Concepts}

\noindent Before getting into the formal discussion, we will introduce some notations and concepts.
\begin{itemize}
	\item $S^{\backslash i}:=S\backslash\left\{(\mathbf{x}_{i},y_{i})\right\}$ be the sample where the $i$-th observation is removed.
	\item $S^{i}:=S^{\backslash i}\cup\{(\mathbf{x},y)\}$ be the sample where the $i$-th observation is replaced by $(\mathbf{x},y)$.
\end{itemize}
and let $\hat{f}_{\backslash i}$ be the estimated result based on sample $S^{\backslash i}$, $\hat{f}_{i}$ based on sample $S^{i}$ and $\hat{f}$ based on sample $S$.

In order to quantify the stability of our algorithm, we will introduce one important concepts --- \textbf{Uniform Stability}.
\begin{definition}[Uniform Stability]
	\label{def:uniform-stability}
	The algorithm is uniformly $\beta$-stable with respect to the loss function $L\left(y,f(\mathbf{x})\right)$, if for all samples $S:=\left\{\mathbf{x}_{i},y_{i}\right\}_{i=1}^{n}\subset\mathcal{D}$ and $i\in[n]$,
	\begin{equation}
		\sup_{(x,y)\in\mathcal{D}}\left|L\left(y,\hat{f}(\mathbf{x})\right)-L\left(y,\hat{f}_{\backslash i}(\mathbf{x})\right)\right|\leq\beta
	\end{equation}
	i.e. the algorithm is "stable" with respect to removing a single sample at all points.
\end{definition}

\subsection{Uniform Stability of Regularized Kernel Model}

Firstly, we will provide an auxiliary lemma.

\begin{lemma}[Convex Functions and Derivatives] \label{lem:convex-functions-and-derivatives}
	For any differentiable convex function $f:\mathbb{R}\rightarrow\mathbb{R}$ and any $a,b\in\mathbb{R}$, we have
	\begin{equation}
		\label{eq:convex-functions-and-derivatives}
		\left[f^{\prime}(a)-f^{\prime}(b)\right](a-b)\geq 0
	\end{equation}
\end{lemma}

\begin{proof}
	Due to the convexity of $f$ we know that $f(a)+(b-a) f^{\prime}(a) \leq f(b)$ and, likewise, $f(b)+(a-b) f^{\prime}(b) \leq f(a)$. Summing up both inequalities and subtracting the terms in $f(a)$ and $f(b)$ proves \eqref{eq:convex-functions-and-derivatives}.
\end{proof}

Then, we will show that the algorithm we studied in this paper satisfied definition \ref{def:uniform-stability}, and the corresponding value of $\beta$ can be calculated.

\begin{theorem}[Algorithmic Stability of Risk Minimizers \cite{bousquet_stability_2002,scholkopf_learning_2001}]
	\label{thm:algorithmic-stability-of-risk-minimizers}
	The algorithm that minimizing the regularized empirical risk in \eqref{eq:regularized-risk-of-rkhs} has stability
	\begin{equation}
		\beta=\frac{2C^{2}\kappa^{2}}{n\lambda}
	\end{equation}
	where $\kappa$ is a bound on $\|k(x,\cdot)\|=\sqrt{k(x,x)}$, $\|\cdot\|$ is the RKHS norm induced by $k$, and $C$ is a bound on the Lipschitz constant of the loss function $L\left(y,f(\mathbf{x})\right)$, which can be viewed as a function of $f$.
\end{theorem}

\begin{remark}
	We can see that the stability of the algorithm depends on the regularization constant via $\frac{1}{\lambda n}$, hence we may be able to afford to choose weaker regularization if the sample size $n$ increases.
\end{remark}

\begin{proof}
	In order to distinguish between different training sets, we use $R_{\text{reg}}[f,S]$ and $R_{\text{reg}}[f,S^{\backslash i}]$ (and likewise $R_{\text{emp}}[f,S]$) during the remainder of the proof.

	Since $\hat{f}$ minimizes $R_{\text{reg}}[f,S]$, that is, the \textbf{functional derivative} \cite{stephane_canu_lecture_2014} of $R_{\text{reg}}[f,S]$ at $\hat{f}$ vanishes, and so does $R_{\text{reg}}[f,S^{\backslash i}]$ at $\hat{f}_{\backslash i}$,
	\begin{equation}
		\label{eq:vanishes-derivative-of-R-reg}
		\begin{aligned}
			\partial_{f}R_{\text{reg}}\left[\hat{f},S\right]=                               & \partial_{f}R_{\text{emp}}\left[\hat{f},S\right]+\lambda\hat{f}=0                                              \\
			\partial_{f}R_{\text{reg}}\left[\hat{f}_{\backslash i},S^{\backslash i}\right]= & \partial_{f}R_{\text{emp}}\left[\hat{f}_{\backslash i},S^{\backslash i}\right]+\lambda\hat{f}_{\backslash i}=0
		\end{aligned}
	\end{equation}

	Next, we construct an auxiliary risk function $\tilde{R}[f]$ by
	\begin{equation}
		\tilde{R}[f]:=\left\langle\partial_{f}R_{\text{emp}}\left[\hat{f},S\right]-\partial_{f}R_{\text{emp}}\left[\hat{f}_{\backslash i},S^{\backslash i}\right],f-\hat{f}_{\backslash i}\right\rangle+\frac{\lambda}{2}\left\|f-\hat{f}_{\backslash i}\right\|_{\mathcal{H}}^{2}
	\end{equation}
	Clearly $\tilde{R}[f]$ is a convex function in $f$ (the first term is linear, the second quadratic).

	Additionally, by construction, we have
	\begin{equation}
		\tilde{R}[\hat{f}_{\backslash i}]=0
	\end{equation}

	Furthermore, taking the functional derivative of $\tilde{R}[f]$, that,
	\begin{equation}
		\label{eq:derviative-of-tilde-R}
		\partial_{f}\tilde{R}[f]=\partial_{f}R_{\text{emp}}\left[\hat{f},S\right]-\partial_{f}R_{\text{emp}}\left[\hat{f}_{\backslash i},S^{\backslash i}\right]+\lambda\left(f-\hat{f}_{\backslash i}\right)=\partial_{f}R_{\text{emp}}\left[\hat{f},S\right]+\lambda f
	\end{equation}
	the functional derivative of $\tilde{R}[f]$ \eqref{eq:derviative-of-tilde-R} vanishes at $f=\hat{f}$ due to \eqref{eq:vanishes-derivative-of-R-reg}, thus the minimum of $\tilde{R}[f]$ is obtained for $f=\hat{f}$. Therefore, combined with $\tilde{R}[\hat{f}_{\backslash i}]=0$, we can conclude that $\tilde{R}[\hat{f}]\leq 0$.

	In order to obtain bounds on $\left\|\hat{f}-\hat{f}_{\backslash i}\right\|$, we have to get rid of some of the first terms in $\tilde{R}[f]$, since
	\begin{equation}
		\begin{aligned}
			     & n\left\langle\partial_{f}R_{\text{emp}}\left[\hat{f},S\right]-\partial_{f}R_{\text{emp}}\left[\hat{f}_{\backslash i},S^{\backslash i}\right],\hat{f}-\hat{f}_{\backslash i}\right\rangle                                       \\
			=    & \sum_{j\neq i}\left[L^{\prime}\left(y_{j},\hat{f}(\mathbf{x}_{j})\right)-L^{\prime}\left(y_{j},\hat{f}_{\backslash i}(\mathbf{x}_{j})\right)\right]\left[\hat{f}(\mathbf{x}_{j})-\hat{f}_{\backslash i}(\mathbf{x}_{j})\right] \\
			     & +L^{\prime}\left(y_{i},\hat{f}(\mathbf{x}_{i})\right)\left[\hat{f}(\mathbf{x}_{i})-\hat{f}_{\backslash i}(\mathbf{x}_{i})\right]                                                                                               \\
			\geq & L^{\prime}\left(y_{i},\hat{f}(\mathbf{x}_{i})\right)\left[\hat{f}(\mathbf{x}_{i})-\hat{f}_{\backslash i}(\mathbf{x}_{i})\right]
		\end{aligned}
	\end{equation}
	The first equation is due to the fact that the functional derivative $\partial_{f}(f)=k(\mathbf{x},.)$ and then collecting the common terms between $R_{\text{emp}}\left[\hat{f},S\right]$ and $R_{\text{emp}}\left[\hat{f}_{\backslash i},S^{\backslash i}\right]$. And, as for the last inequation, we use lemma \ref{lem:convex-functions-and-derivatives} applied to the loss function $L\left(y,f(\mathbf{x})\right)$ which is a convex function of $f(\mathbf{x})$.

	Combine the above result with the fact $\tilde{R}[\hat{f}]\leq 0$, we have
	\begin{equation}
		\left\langle\partial_{f}R_{\text{emp}}\left[\hat{f},S\right]-\partial_{f}R_{\text{emp}}\left[\hat{f}_{\backslash i},S^{\backslash i}\right],\hat{f}-\hat{f}_{\backslash i}\right\rangle+\frac{\lambda}{2}\left\|\hat{f}-\hat{f}_{\backslash i}\right\|_{\mathcal{H}}^{2}\leq 0
	\end{equation}
	thus,
	\begin{equation}
		L^{\prime}\left(y_{i},\hat{f}(\mathbf{x}_{i})\right)\left[\hat{f}(\mathbf{x}_{i})-\hat{f}_{\backslash i}(\mathbf{x}_{i})\right]+\frac{n\lambda}{2}\left\|\hat{f}-\hat{f}_{\backslash i}\right\|_{\mathcal{H}}^{2}\leq 0
	\end{equation}
	and by the convexity of loss function $L\left(y,f(\mathbf{x})\right)$,
	\begin{equation}
		\label{eq:rkhs-norm-by-derviative-of-tilde-R}
		\begin{aligned}
			\frac{n\lambda}{2}\left\|\hat{f}-\hat{f}_{\backslash i}\right\|_{\mathcal{H}}^{2}\leq & L^{\prime}\left(y_{i},\hat{f}(\mathbf{x}_{i})\right)\left[\hat{f}_{\backslash i}(\mathbf{x}_{i})-\hat{f}(\mathbf{x}_{i})\right] \\
			\leq                                                                                  & L\left(y_{i},\hat{f}(\mathbf{x}_{i})\right)-L\left(y_{i},\hat{f}_{\backslash i}(\mathbf{x}_{i})\right)                          \\
			\leq                                                                                  & \left|L\left(y_{i},\hat{f}(\mathbf{x}_{i})\right)-L\left(y_{i},\hat{f}_{\backslash i}(\mathbf{x}_{i})\right)\right|
		\end{aligned}
	\end{equation}

	By the Cauchy-Schwarz inequality we can see that, for any $f,f^{\prime}\in\mathcal{H}$ and any $\mathbf{x}\in\mathcal{X}$,
	\begin{equation}
		\left|f(\mathbf{x})-f^{\prime}(\mathbf{x})\right|=\left|\left\langle f-f^{\prime},k(\mathbf{x},\cdot)\right\rangle\right|\leq\left\|f-f^{\prime}\right\|_{\mathcal{H}}\left\|k(\mathbf{x},\cdot)\right\|_{\mathcal{H}}\leq\kappa\left\|f-f^{\prime}\right\|_{\mathcal{H}}
	\end{equation}
	and since $L\left(y,f(\mathbf{x})\right)$ is Lipschitz continuous at $\mathbf{x}_{i}$,  we have
	\begin{equation}
		\label{eq:rkhs-norm-by-lipshitz-continuous}
		\left|L\left(y,\hat{f}(\mathbf{x}_{i})\right)-L\left(y,\hat{f}_{\backslash i}(\mathbf{x}_{i})\right)\right|\leq C\left|\hat{f}(\mathbf{x}_{i})-\hat{f}_{\backslash i}(\mathbf{x}_{i})\right|\leq C\kappa\left\|\hat{f}-\hat{f}_{\backslash i}\right\|_{\mathcal{H}}
	\end{equation}

	Combine equation \eqref{eq:rkhs-norm-by-derviative-of-tilde-R} and \eqref{eq:rkhs-norm-by-lipshitz-continuous}, we get
	\begin{equation}
		\frac{n\lambda}{2}\left\|\hat{f}-\hat{f}_{\backslash i}\right\|_{\mathcal{H}}^{2}\leq C\kappa\left\|\hat{f}-\hat{f}_{\backslash i}\right\|_{\mathcal{H}}
	\end{equation}
	thus,
	\begin{equation}
		\left\|\hat{f}-\hat{f}_{\backslash i}\right\|_{\mathcal{H}}\leq\frac{2C\kappa}{n\lambda}
	\end{equation}

	Therefore, by the equation \eqref{eq:rkhs-norm-by-lipshitz-continuous} for every $\mathbf{x}$, we have
	\begin{equation}
		\left|L\left(y,\hat{f}(\mathbf{x})\right)-L\left(y,\hat{f}_{\backslash i}(\mathbf{x})\right)\right|\leq C\kappa\left\|\hat{f}-\hat{f}_{\backslash i}\right\|_{\mathcal{H}}\leq\frac{2C^{2}\kappa^{2}}{n\lambda}
	\end{equation}
\end{proof}

Within the uniform stability of our algorithm, we will also prove that the $\beta$-stable algorithm exhibit uniform convergence of the empirical risk $R_{\text{emp}}[f]$ to the true risk $R[f]$.

\begin{theorem}[McDiarmid's Bound \cite{siemons_method_1989}] \label{thm:McDiarmid-bound}
	Suppose $\xi_{1},\ldots,\xi_{n}$ be i.i.d real value random variables and assume that there exists a function $g:\mathbb{R}^{n}\rightarrow\mathbb{R}$ with the property that for all $i\in[n]$ and $c_{i}>0$,
	\begin{equation}
		\sup_{\xi_{1},\ldots,\xi_{n},\xi_{i}^{\prime}\in\xi}\left|g\left(\xi_{1},\ldots,\xi_{n}\right)-g\left(\xi_{1},\ldots,\xi_{i-1},\xi_{i}^{\prime},\xi_{i+1},\ldots,\xi_{n}\right)\right|\leq c_{i}
	\end{equation}
	where $\xi_{i}^{\prime}$ is drawn from the same distribution as $\xi_{i}$. Then
	\begin{equation}
		\mathbb{P}\left\{\left|g\left(\xi_{1},\ldots,\xi_{n}\right)-\mathbb{E}\left[g\left(\xi_{1},\ldots,\xi_{n}\right)\right]\right|>\varepsilon\right\}\leq 2\exp\left(-\frac{2\varepsilon^{2}}{\sum_{i=1}^{n}c_{i}^{2}}\right)
	\end{equation}
\end{theorem}

\begin{theorem}[Bousquet and Elisseeff \cite{bousquet_algorithmic_2001,ofer_dekel_algorithmic_2011}] \label{thm:bousquet-and-elisseeff}
	Assume that we have a $\beta$-stable algorithm with the additional requirement that the loss function $L\left(y,f(\mathbf{x})\right)\leq M$ for all $(\mathbf{x},y)\in\mathcal{D}$ and for all samples $S:=\left\{\mathbf{x}_{i},y_{i}\right\}_{i=1}^{n}\subset\mathcal{D}$. Then, for any $n\geq 1$
	\begin{equation}
		\mathbb{P}\left\{\left|R_{\text{emp}}[\hat{f},S]-R[\hat{f}]\right|>\varepsilon+2\beta\right\}\leq 2\exp\left(-\frac{n\varepsilon^{2}}{2(n\beta+M)^{2}}\right)
	\end{equation}
\end{theorem}

\begin{proof}
	Within the i.i.d assumption, we have
	\begin{equation}
		\begin{aligned}
			\mathbb{E}_{S\sim\mathcal{D}}\left[R_{\text{emp}}[\hat{f}]\right] & =\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{S\sim\mathcal{D}}\left[L\left(y_{i},\hat{f}(\mathbf{x}_{i})\right)\right]=\mathbb{E}_{S\sim\mathcal{D}}\left[L\left(y_{i},\hat{f}(\mathbf{x}_{i})\right)\right]
		\end{aligned}
	\end{equation}
	If we replace $(\mathbf{x}_{i},y_{i})$ by $(\mathbf{x},y)$, we can get
	\begin{equation}
		\mathbb{E}_{S\sim\mathcal{D}}\left[R_{\text{emp}}[\hat{f}]\right]=\mathbb{E}_{S,\left(\mathbf{x},y\right)\sim\mathcal{D}}\left[L\left(y,\hat{f}_{i}(\mathbf{x})\right)\right]
	\end{equation}
	and with the observation that
	\begin{equation}
		\mathbb{E}_{\mathcal{D}}\left[R[\hat{f}]\right]=\mathbb{E}_{S,\left(\mathbf{x},y\right)\sim\mathcal{D}}\left[L\left(y,\hat{f}(\mathbf{x})\right)\right]
	\end{equation}
	In order to bound on the expected difference between $R_{\text{emp}}[\hat{f},S]$ and $R[\hat{f}]$, which leads to
	\begin{equation}
		\begin{aligned}
			\mathbb{E}_{\mathcal{D}}\left[R_{\text{emp}}[\hat{f},S]-R[\hat{f}]\right]= & \mathbb{E}_{S,\left(\mathbf{x},y\right)\sim\mathcal{D}}\left[L\left(y,\hat{f}_{i}(\mathbf{x})\right)\right]-\mathbb{E}_{S,\left(\mathbf{x},y\right)\sim\mathcal{D}}\left[L\left(y,\hat{f}(\mathbf{x})\right)\right] \\
			=                                                                          & \mathbb{E}_{S,\left(\mathbf{x},y\right)\sim\mathcal{D}}\left[L\left(y,\hat{f}_{i}(\mathbf{x})\right)-L\left(y,\hat{f}(\mathbf{x})\right)\right]\leq 2\beta
		\end{aligned}
	\end{equation}

	By the triangle inequality, we have
	\begin{equation}
		\left|R[\hat{f}]-R[\hat{f}_{i}]\right|\leq\left|R[\hat{f}]-R[\hat{f}_{\backslash i}]\right|+\left|R[\hat{f}_{\backslash i}]-R[\hat{f}_{i}]\right|\leq 2\beta
	\end{equation}
	Also, we have
	\begin{equation}
		\begin{aligned}
			\left|R_{\text{emp}}[\hat{f},S]-R_{\text{emp}}\left(\hat{f}_{i},S^{i}\right)\right|\leq & \frac{1}{n}\sum_{j\neq i}\left|L\left(y_{j},\hat{f}(\mathbf{x}_{j})\right)-L\left(y_{j},\hat{f}_{i}(\mathbf{x}_{j})\right)\right| \\
			                                                                                        & +\frac{1}{n}\left|L\left(y_{i},\hat{f}(\mathbf{x}_{i})\right)-L\left(y_{i},\hat{f}_{i}(\mathbf{x}_{i})\right)\right|              \\
			\leq                                                                                    & \frac{n-1}{n}2\beta+\frac{2M}{n}\leq 2\beta+\frac{2M}{n}
		\end{aligned}
	\end{equation}
	and,
	\begin{equation}
		\begin{aligned}
			\left|\left[R_{\text{emp}}[\hat{f},S]-R[\hat{f}]\right]-\left[R_{\text{emp}}\left(\hat{f}_{i},S^{i}\right)-R[\hat{f}_{i}]\right]\right|\leq & \left|R_{\text{emp}}[\hat{f},S]-R_{\text{emp}}\left(\hat{f}_{i},S^{i}\right)\right| \\
			                                                                                                                                            & +\left|R[\hat{f}]-R[\hat{f}_{i}]\right|                                             \\
			\leq                                                                                                                                        & 4\beta+\frac{2M}{n}
		\end{aligned}
	\end{equation}

	Thus, by the Theorem \ref{thm:McDiarmid-bound}, we have $c_{i}=4\beta+\frac{2M}{n}$, that,
	\begin{equation}
		\begin{aligned}
			\mathbb{P}\left\{|R_{\text{emp}}[\hat{f},S]-R[\hat{f}]-2\beta|>\varepsilon\right\}\leq & \mathbb{P}\left\{|R_{\text{emp}}[\hat{f},S]-R[\hat{f}]|>\varepsilon+2\beta\right\} \\
			\leq                                                                                   & 2\exp\left(-\frac{n\varepsilon^{2}}{2(2n\beta+M)^{2}}\right)
		\end{aligned}
	\end{equation}
\end{proof}

Within the above two theorems, we can directly get the following practical consequence.
\begin{corollary}[Uniform Convergence Bounds for RKHS]
	The algorithm minimizing the regularized risk $R_{\text{reg}}[f]$, as in \eqref{eq:regularized-risk-of-rkhs}, and with the assumptions of Theorem \ref{thm:algorithmic-stability-of-risk-minimizers} and \ref{thm:bousquet-and-elisseeff}, we obtain
	\begin{eqnarray}
		\label{eq:uniform-convergence-bounds-for-rkhs}
		\mathbb{P}\left\{\left|R_{\text{emp}}[\hat{f}]-R[\hat{f}]\right|>\varepsilon+2\beta\right\}\leq 2\exp\left(-\frac{n}{2}\left(\frac{\varepsilon}{M}\right)^{2}\left(1+\frac{4}{\lambda M}(C \kappa)^{2}\right)^{-2}\right)
	\end{eqnarray}
	where
	\begin{equation*}
		\beta=\frac{2C^{2}\kappa^{2}}{n\lambda}
	\end{equation*}
\end{corollary}

\begin{remark}
	For practical considerations, \eqref{eq:uniform-convergence-bounds-for-rkhs} may be very useful, even if the rates are not optimal, since the bound is predictive even for small sample sizes and moderate regularization strength. Still, we expect that the constants
\end{remark}

\subsection*{Comments}

The idea of the discussion content was inspired by \cite[Section 3]{hofmann_kernel_2008} review, and the report is organized follow \cite[Chapter 12]{scholkopf_learning_2001} structure, and so does the main proof ideas.
