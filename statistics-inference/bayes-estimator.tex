\chapter{Bayes Estimator}

We shall look for some estimators that make the risk function $R\left(\theta,\delta\right)$ small in some overall sense. There are two way to solve it: minimize the average risk, minimize the maximum risk.

This chapter will discuss the first method, also known as, Bayes Estimator.

\begin{definition}{Bayes Estimator}{bayes-estimator}
    The Bayes Estimator $\delta$ with respect to $\Lambda$ is minimizing the Bayes Risk of $\delta$
    \begin{equation}
        r\left(\Lambda, \delta\right)=\int R\left(\theta, \delta\right) \mathrm{d} \Lambda\left(\theta\right)
    \end{equation}
    where $\Lambda$ is the probability distribution.
\end{definition}

In Bayesian arguments, it is important to keep track of which variables are being conditioned on. Hence, the notations are as followed:
\begin{itemize}
    \item The density of $X$ will be denoted by $X \sim f\left(x \mid \theta\right)$.
    \item The prior distribution will be denoted by $\Pi \sim \pi\left(\theta \mid \lambda\right)$ or $\Lambda \sim \gamma\left(\lambda\right)$, where $\lambda$ is another parameter (sometimes called a hyperparameter).
    \item The posterior distribution, which calculate the conditional distributions as that of $\theta$ given $x$ and $\lambda$, or $\lambda$ given $x$, which is denoted by $\Pi \sim \pi\left(\theta \mid x, \lambda\right)$ or $\Lambda \sim \gamma\left(\lambda \mid x\right)$, that is
    \begin{equation}
        \pi\left(\theta \mid x, \lambda\right) = \frac{f\left(x \mid \theta\right) \pi\left(\theta \mid \lambda\right)}{m\left(x \mid \lambda\right)},
    \end{equation}
    where marginal distributions $m\left(x \mid \lambda\right) = \int f\left(x \mid \theta\right) \pi\left(\theta \mid \lambda\right) \mathrm{d} \theta$.
\end{itemize}

\begin{theorem}{}{bayes-definition}
    Let $\Theta$ have distribution $\Lambda$, and given $\Theta=\theta$, let $X$ have distribution $P_{\theta}$. Suppose, the following assumptions hold for the problem of estimating $g\left(\Theta\right)$ with non-negative loss function $L\left(\theta,d\right)$,
    \begin{itemize}
        \item There exists an estimator $\delta_0$ with finite risk.
        \item For almost all $x$, there exists a value $\delta_{\Lambda}\left(x\right)$ minimizing
        \begin{equation}
            E\{L[\Theta,\delta\left(x\right)] \mid X=x\}.
        \end{equation}
    \end{itemize}
    Then, $\delta_{\Lambda}\left(x\right)$ is a Bayes Estimator.
\end{theorem}
\begin{note}
    Improper prior
\end{note}

\begin{corollary}{}{}
    Suppose the assumptions of Theorem \ref{thm:bayes-definition} hold.
    \begin{enumerate}
        \item If $L\left(\theta,d\right)=[d-g\left(\theta\right)]^2$, then
        \begin{equation}
            \delta_{\Lambda}\left(x\right)=E[g\left(\Theta\right) \mid x].
        \end{equation}
        \item If $L\left(\theta,d\right)=w\left(\theta\right)[d-g\left(\theta\right)]^2$, then
        \begin{equation}
            \delta_{\Lambda}\left(x\right)=\frac{E[w\left(\theta\right)g\left(\Theta\right) \mid x]}{E[w\left(\theta\right) \mid x]}.
        \end{equation}
        \item If $L\left(\theta,d\right)=|d-g\left(\theta\right)|$, then $\delta_\Lambda\left(x\right)$ is any median of the conditional distribution of $\Theta$ given $x$.
        \item If
        \begin{equation*}
            L(\theta, d)=\left\{
                \begin{array}{l}
                    0 \text { when }|d-\theta| \leq c \\
                    1 \text { when }|d-\theta|>c
                \end{array}
            \right.,
        \end{equation*}
        then $\delta_\Lambda\left(x\right)$ is the midpoint of the interval $I$ of length $2c$ which maxmizes $P\left(\Theta\in I\mid x\right)$.
    \end{enumerate}
\end{corollary}

\begin{proof}
    
\end{proof}

\begin{theorem}{}{}
    Necessiary condition for Bayes Estimator
\end{theorem}

Methodologies have been developed to deal with the difficulty which sometimes incorporate frequentist measures to assess the choic of $\Lambda$.

\begin{itemize}
    \item Empirical Bayes.
    \item Hierarchical Bayes.
    \item Robust Bayes.
    \item Objective Bayes.
\end{itemize}

\section{Single-Prior Bayes}

The Single-Prior Bayes model in a general form as
\begin{equation}
    \begin{aligned}
        X\mid\theta &\sim f\left(x\mid\theta\right),\\
        \Theta\mid\gamma &\sim \pi\left(\theta\mid\lambda\right),
    \end{aligned}
    \label{eq:single-prior-bayes}
\end{equation}
where we assume that the functional form of the prior and the value of $\lambda$ is known (we will write it as $\gamma=\gamma_0)$.

Given a loss function $L\left(\theta,d\right)$, we would then determine the estimator that minimizes
\begin{equation}
    \int L\left(\theta,d\left(x\right)\right)\pi\left(\theta\mid x\right)\mathrm{d}\theta,
\end{equation}
where $\pi\left(\theta\mid x\right)$ is posterior distribution given by
\begin{equation*}
    \pi\left(\theta\mid x\right)=\frac{f\left(x\mid\theta\right)\pi\left(\theta\mid\gamma_0\right)}{\int f\left(x\mid\theta\right)\pi\left(\theta\mid\gamma_0\right)\mathrm{d}\theta}.
\end{equation*}

In general, this Bayes estimator under squared error loss is given by
\begin{equation}
    E\left(\Theta\mid x\right) = \frac{\int\theta f\left(x\mid\theta\right)\pi\left(\theta\mid\gamma_0\right)\mathrm{d}\theta}{\int f\left(x\mid\theta\right)\pi\left(\theta\mid\gamma_0\right)\mathrm{d}\theta}.
\end{equation}

For exponential families

\begin{theorem}{}{}
    
\end{theorem}

\section{Hierarchical Bayes}

In a Hierarchical Bayes model, rather than specifying the prior distribution as a single function, we specify it in a \textbf{hierarchy}. Thus, the Hierarchical Bayes model in a general form as
\begin{equation}
    \begin{aligned}
        X\mid\theta &\sim f\left(x\mid\theta\right),\\
        \Theta\mid\gamma &\sim \pi\left(\theta\mid\lambda\right),\\
        \Gamma &\sim \psi\left(\gamma\right),
    \end{aligned}
    \label{eq:hierarchical-bayes}
\end{equation}
where we assume that $\psi\left(\cdot\right)$ is known and not dependent on any other unknown hyperparameters.

\begin{note}
    We can continue this hierarchical modeling and add more stages to the model, but this is not ofthen done in practice.
\end{note}

Given a loss function $L\left(\theta,d\right)$, we would then determine the estimator that minimizes
\begin{equation}
    \int L\left(\theta,d\left(x\right)\right)\pi\left(\theta\mid x\right)\mathrm{d}\theta,
    \label{eq:hierarchical-bayes-estimator}
\end{equation}
where $\pi\left(\theta\mid x\right)$ is posterior distribution given by
\begin{equation*}
    \pi\left(\theta\mid x\right)=\frac{\int f\left(x\mid\theta\right)\pi\left(\theta\mid\gamma\right)\psi\left(\gamma\right)\mathrm{d}\gamma}{\int\int f\left(x\mid\theta\right)\pi\left(\theta\mid\gamma\right)\psi\left(\gamma\right)\mathrm{d}\theta\mathrm{d}\gamma}.
\end{equation*}

\begin{note}
    The posterior distribution can also be writed as
    \begin{equation*}
        \pi\left(\theta\mid x\right)=\int\pi\left(\theta\mid x,\gamma\right)\pi\left(\gamma\mid x\right)\mathrm{d}\gamma,
    \end{equation*}
    where $\pi\left(\gamma\mid x\right)$ is the posterior distribution of $\Gamma$, unconditional on $\theta$. The equation \ref{eq:hierarchical-bayes-estimator} can be writed as
    \begin{equation*}
        \int L\left(\theta,d\left(x\right)\right)\pi\left(\theta\mid x\right)\mathrm{d}\theta = \int\left[\int L\left(\theta,d\left(x\right)\right)\pi\left(\theta\mid x,\gamma\right)\mathrm{d}\theta\right]\pi\left(\gamma\mid x\right)\mathrm{d}\gamma.
    \end{equation*}
    which shows that \textbf{the Hierarchical Bayes estimator can be thought of as a mixture of Single-Prior estimators}.
\end{note}

\begin{example}[ Poisson Hierarchy ]
    Consider
    \begin{equation}
        \begin{aligned}
            X\mid\lambda &\sim \text{Poisson}\left(\lambda\right),\\
            \Lambda\mid b &\sim \text{Gamma}\left(a,b\right), \text{ a known},\\
            \frac{1}{b} &\sim \text{Gamma}\left(k,\tau\right),
        \end{aligned}
    \end{equation}
    calculate the Hierarchical Bayes estimator under squared error loss.
\end{example}


\begin{theorem}{}{}
    For the Hierarchical Bayes model (\ref{eq:hierarchical-bayes}),
    \begin{equation}
        K\left[\pi\left(\lambda\mid x\right),\psi\left(\lambda\right)\right] < K\left[\pi\left(\theta\mid x\right),\pi\left(\theta\right)\right],
    \end{equation}
    where $K$ is the Kullback-Leibler information for discrimination between two densities.
\end{theorem}

\begin{proof}
    
\end{proof}

\begin{note}
    
\end{note}

\section{Empirical Bayes}

\section{Bayes Prediction}