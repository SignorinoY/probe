\chapter{Frequentist Inference}

\section{Point Estimation}

\subsection{Minimum-Variance Unbiased Estimator}

\begin{definition}[UMVU Estimators]
    An unbiased estimator $\delta(\textbf{X})$ of $g(\theta)$ is the uniform minimum variance unbiased (UMVU) estimator of $g(\theta)$ if
    \begin{equation}
        \text{Var}_{\theta}\delta(\textbf{X})\leq\text{Var}_{\theta}\delta'(\textbf{X}),\quad\forall\theta\in\Theta,
    \end{equation}
    where $\delta'(\textbf{X})$ is any other unbiased estimator of $g(\theta)$.
\end{definition}

\begin{remark}
    If there exists an unbiased estimator of $g$, the estimand $g$ will be called $U$-estimable.
\end{remark}

\begin{enumerate}
    \item If $T(\textbf{X})$ is a complete sufficient statistic, estimator $\delta(\textbf{X})$ that only depends on $T(\textbf{X})$, then for any $U$-estimable function $g(\theta)$ with
          \begin{equation}
              E_{\theta}\delta(T(\textbf{X}))=g(\theta),\quad\forall\theta\in\Theta,
          \end{equation}
          hence, $\delta(T(\textbf{X}))$ is the unique UMVU estimator of $g(\theta)$.
    \item If $T(\textbf{X})$ is a complete sufficient statistic and $\delta({\textbf{X}})$ is any unbiased estimator of $g(\theta)$, then the UMVU estimator of $g(\theta)$ can be obtained by
          \begin{equation}
              E\left[\delta(\textbf{X})\mid T(\textbf{X})\right].
          \end{equation}
\end{enumerate}

\begin{example}[Estimating Polynomials of a Normal Variance]
    Let $X_{1},\ldots,X_{n}$ be distributed with joint density
    \begin{equation}
        \frac{1}{(\sqrt{2\pi}\sigma)^{n}}\exp\left[-\frac{1}{2\sigma^{2}}\sum\left(x_{i}-\xi\right)^{2}\right].
    \end{equation}
    Discussing the UMVU estimators of $\xi^r$, $\sigma^r$, $\xi/\sigma$.
\end{example}

\begin{proof}
    \begin{enumerate}
        \item \textbf{$\sigma$ is known}:

              Since $\bar{X}=\frac{1}{n}\sum_{i=1}^{n}X_i$ is the complete sufficient statistic of $X_i$, and
              \begin{equation*}
                  E(\bar{X})=\xi,
              \end{equation*}
              then the UMVU estimator of $\xi$ is $\bar{X}$.

              Therefore, the UMVU estimator of $\xi^r$ is $\bar{X}^r$ and the UMVU estimator of $\xi/\sigma$ is $\bar{X}/\sigma$.

        \item \textbf{$\xi$ is known}:

              Since $s^r=\sum\left(x_{i}-\xi\right)^r$ is the complete sufficient statistic of $X_i$.

              Assume
              \begin{equation*}
                  E\left[\frac{s^r}{\sigma^r}\right]=\frac{1}{K_{n,r}},
              \end{equation*}
              where $K_{n,r}$ is a constant depends on $n,r$.

              Since $s^2/\sigma^2\sim\text{Ga}(n/2,1/2)=\chi^2(n)$, then
              \begin{equation*}
                  E\left[\frac{s^r}{\sigma^r}\right]=E\left[\left(\frac{s^2}{\sigma^2}\right)^{\frac{r}{2}}\right]=\int_{0}^{\infty}x^{\frac{r}{2}}\frac{1}{2^{\frac{n}{2}}\Gamma(\frac{n}{2})}x^{\frac{n}{2}-1}e^{-\frac{x}{2}}\mathrm{d}x=\frac{\Gamma\left(\frac{n+r}{2}\right)}{\Gamma(\frac{n}{2})}\cdot 2^{\frac{r}{2}}.
              \end{equation*}
              therefore,
              \begin{equation*}
                  K_{n,r}=\frac{\Gamma(\frac{n}{2})}{2^{\frac{r}{2}}\cdot\Gamma\left(\frac{n+r}{2}\right)}.
              \end{equation*}

              Hence,
              \begin{equation*}
                  E\left[s^rK_{n,r}\right]=\sigma^r \text{ and } E[\xi s^{-1}K_{n,-1}]=\xi/\sigma,
              \end{equation*}
              which means the UMVU estimator of $\sigma^r$ is $s^rK_{n,r}$ and the UMVU estimator of $\xi/\sigma$ is $\xi s^{-1}K_{n,-1}$.

        \item \textbf{Both $\xi$ and $\sigma$ is unknown}:

              Since $(\bar{X},s_x^r)$ are the complete sufficient statistic of $X_i$, where $s_x^2=\sum\left(x_{i}-\bar{X}\right)^r$.

              Since $s_x^2/\sigma^2\sim\chi^2(n-1)$, then
              \begin{equation*}
                  E\left[\frac{s_x^r}{\sigma^r}\right]=\frac{1}{K_{n-1,r}}.
              \end{equation*}

              Hence,
              \begin{equation*}
                  E\left[s_x^rK_{n-1,r}\right]=\sigma^r,
              \end{equation*}
              which means the UMVU estimator of $\sigma^r$ is $s_x^rK_{n-1,r}$,
              and
              \begin{equation*}
                  E(\bar{X}^r)=\xi^r,
              \end{equation*}
              which means the UMVU estimator of $\xi^r$ is $\bar{X}^r$.

              Since $\bar{X}$ and $s_x^r$ are independent, then
              \begin{equation*}
                  E[\bar{X}s_x^{-1}K_{n-1,-1}]=\xi/\sigma
              \end{equation*}
              which means the UMVU estimator of $\xi/\sigma$ is $\bar{X}s_x^{-1}K_{n-1,-1}$.
    \end{enumerate}
\end{proof}

\begin{example}[]
    Let $X_{1},\ldots,X_{n}$ be i.i.d sample from $U\left(\theta_1-\theta_2,\theta_1+\theta_2\right)$, where $\theta_1\in\mathbb{R},\theta_2\in\mathbb{R}^+$. Discussing the UMVU estimators of $\theta_1,\theta_2$.
\end{example}

\begin{proof}
    Let $X_{(i)}$ be the i-th order statistic of $X_i$, then $\left(X_{(1)},X_{(n)}\right)$ is the complete and sufficient statistic for $(\theta_1,\theta_2)$. Thus it suffices to find a function $\left(X_{(1)},X_{(n)}\right)$, which is unbiased of $(\theta_1,\theta_2)$.

    Let
    \begin{equation*}
        Y_i=\frac{X_i-(\theta_1-\theta_2)}{2\theta_2}\sim U(0,1),
    \end{equation*}
    and
    \begin{equation*}
        Y_{(i)}=\frac{X_{(i)}-(\theta_1-\theta_2)}{2\theta_2},
    \end{equation*}
    be the i-th order statistic of $Y_i$, then we got
    \begin{equation*}
        \begin{aligned}
            E[X_{(1)}] & = 2\theta_2E[Y_{(1)}]+(\theta_1-\theta_2)                           \\
                       & = 2\theta_2\int_{0}^{1}ny(1-y)^{n-1}\mathrm{d}y+(\theta_1-\theta_2) \\
                       & = \theta_1-\frac{3n+1}{n+1}\theta_2                                 \\
            E[X_{(n)}] & = 2\theta_2E[Y_{(n)}]+(\theta_1-\theta_2)                           \\
                       & = 2\theta_2\int_{0}^{1}ny^{n}\mathrm{d}y+(\theta_1-\theta_2)        \\
                       & = \theta_1+\frac{n-1}{n+1}\theta_2                                  \\
        \end{aligned}.
    \end{equation*}

    Thus,
    \begin{equation*}
        \begin{aligned}
            \theta_1 & = E\left[\frac{n-1}{4n}X_{(1)}+\frac{3n+1}{4n}X_{(n)}\right], \\
            \theta_2 & = E\left[-\frac{n+1}{4n}X_{(1)}+\frac{n+1}{4n}X_{(n)}\right], \\
        \end{aligned}
    \end{equation*}
    which means the UMVU estimator is
    \begin{equation*}
        \hat{\theta_1}=\frac{n-1}{4n}X_{(1)}+\frac{3n+1}{4n}X_{(n)},\quad\hat{\theta_2}=-\frac{n+1}{4n}X_{(1)}+\frac{n+1}{4n}X_{(n)}.
    \end{equation*}
\end{proof}

\subsection{Maximum Likelihood Estimator}

Suppose that $\textbf{X}_{n}=\left(X_{1},\ldots,X_{n}\right)$, where the $X_{i}$ are i.i.d. with common density $p\left(x;\theta_{0}\right)\in\mathcal{P}=\{p(x;\theta):\theta\in\Theta\}$.

We assume that

\begin{quotation}
    $\theta_{0}$ is identified in the sense that if $\theta\neq\theta_{0}$ and $\theta\in\Theta$, then $p(x;\theta)\neq p\left(x;\theta_{0}\right)$ with respect to the dominating measure $\mu$.
\end{quotation}

For fixed $\theta\in\Theta$, the joint density of $\textbf{X}_{n}$ is equal to the product of the individual densities, i.e.,

\begin{equation}
    p\left(\textbf{X}_{n};\theta\right)=\prod_{i=1}^{n} p\left(x_{i};\theta\right).
\end{equation}

The maximum likelihood estimate for observed $\textbf{X}_{n}$ is the value $\theta\in\Theta$ which maximizes $L\left(\theta;X_{n}\right):=p\left(\textbf{X}_{n};\theta\right)$, i.e.,

\begin{equation}
    \hat{\theta}\left(\textbf{X}_{n}\right)=\max_{\theta\in\Theta}L\left(\theta;X_{n}\right).
\end{equation}

Equivalently, the MLE can be taken to be the maximum of the standardized log-likelihood,

\begin{equation}
    \frac{l\left(\theta;\textbf{X}_{n}\right)}{n}=\frac{\log L\left(\theta;\textbf{X}_{n}\right)}{n}=\frac{1}{n}\sum_{i=1}^{n}\log p\left(X_{i};\theta\right)=\frac{1}{n}\sum_{i=1}^{n}l\left(\theta;X_{i}\right).
\end{equation}

Define
\begin{equation}
    \begin{gathered}
        Q\left(\theta;\textbf{X}_{n}\right):=\frac{1}{n}\sum_{i=1}^{n}l\left(\theta;X_{i}\right), \\
        \hat{\theta}\left(\textbf{X}_{n}\right):=\max_{\theta\in\Theta}Q\left(\theta;\textbf{X}_{n}\right). \\
    \end{gathered}
\end{equation}

\paragraph{Consistency of MLE}

By the Weak Law of Large Numbers (Theorem \ref{thm:WLLN}), we can get,

\begin{equation}
    \frac{1}{n}\sum^{n}l\left(\theta;X_{i}\right)\stackrel{p}{\rightarrow}E\left[l(\theta;X)\right].
\end{equation}

Suppose $Q_{0}(\theta)=E\left[l(\theta;X)\right]$, then we will show that $Q_{0}(\theta)$ is maximized at $\theta_{0}$ (i.e., the truth).

\begin{lemma}
    If $\theta_{0}$ is identified and $E_{\theta_{0}}\left[|\log p(X;\theta)|\right]<\infty,\forall\theta\in\Theta$, then $Q_{0}(\theta)$ is uniquely maximized at $\theta=\theta_{0}$.
\end{lemma}

\begin{proof}

\end{proof}

\begin{theorem}[Consistency of MLE]
    Suppose that $Q\left(\theta;\textbf{X}_{n}\right)$ is continuous in $\theta$ and there exists a function $Q_{0}(\theta)$ such that
    \begin{enumerate}
        \item $Q_{0}(\theta)$ is uniquely maximized at $\theta_{0}$.
        \item $\Theta$ is compact.
        \item $Q_{0}(\theta)$ is continuous in $\theta$.
        \item $Q\left(\theta;\textbf{X}_{n}\right)$ converges uniformly in probability to $Q_{0}(\theta)$.
    \end{enumerate}
    then
    \begin{equation}
        \hat{\theta}\left(\textbf{X}_{n}\right)\stackrel{p}{\rightarrow}\theta_{0}.
    \end{equation}
\end{theorem}

\begin{proof}
    $\forall\varepsilon>0$, let
    \begin{equation*}
        \Theta(\epsilon)=\left\{\theta:\left\|\theta-\theta_{0}\right\|<\epsilon\right\}.
    \end{equation*}

    Since $\Theta(\epsilon)$ is an open set, then $\Theta\cap\Theta(\epsilon)^{C}$ is a compact set (Assumption 2).

    Since $Q_{0}(\theta)$ is a continuous function (Assumption 3), then
    \begin{equation*}
        \theta^{*}:=\sup_{\theta\in\Theta\cap \Theta(\epsilon)^{C}}\left\{Q_{0}(\theta)\right\}
    \end{equation*}
    is a achieved for a $\theta$ in the compact set.

    Since $\theta_{0}$ is the unique maximized, let
    \begin{equation*}
        Q_{0}\left(\theta_{0}\right)-Q_{0}\left(\theta^{*}\right)=\delta>0.
    \end{equation*}

    \begin{enumerate}
        \item For $\theta\in\Theta\cap\Theta(\epsilon)^{C}$. Let $A_n=\left\{\sup_{\theta\in\Theta\cap\Theta(\epsilon)^{C}}\left|Q\left(\theta;\textbf{X}_{n}\right)-Q_{0}(\theta)\right|<\frac{\delta}{2}\right\}$, then
              \begin{equation*}
                  \begin{aligned}
                      A_{n}\Rightarrow Q\left(\theta;\textbf{X}_{n}\right) & <Q_{0}(\theta)+\frac{\delta}{2}                    \\
                                                                           & \leq Q_{0}\left(\theta^{*}\right)+\frac{\delta}{2} \\
                                                                           & = Q_{0}\left(\theta_{0}\right)-\frac{\delta}{2}
                  \end{aligned}
              \end{equation*}
        \item For $\theta\in\Theta(\epsilon)$. Let $B_n=\left\{\sup_{\theta\in\Theta(\epsilon)}\left|Q\left(\theta;\boldsymbol{X}_{n}\right)-Q_{0}(\theta)\right|<\frac{\delta}{2}\right\}$, then
              \begin{equation*}
                  B_{n}\Rightarrow Q\left(\theta;\boldsymbol{X}_{n}\right)>Q_{0}(\theta)-\frac{\delta}{2},\forall\theta\in\Theta(\epsilon)
              \end{equation*}
              By Assumption 1,
              \begin{equation*}
                  Q\left(\theta_{0};\boldsymbol{X}_{n}\right)>Q_{0}\left(\theta_{0}\right)-\frac{\delta}{2}
              \end{equation*}
    \end{enumerate}

    If both $A_{n}$ and $B_{n}$ hold, then
    \begin{equation*}
        \hat{\theta}\in\Theta(\epsilon).
    \end{equation*}

    By Assumption 4, we can concluded that $P\left(A_{n}\cap B_{n}\right)\rightarrow 1$, so
    \begin{equation*}
        P(\hat{\theta}\in\Theta(\epsilon))\rightarrow 1,
    \end{equation*}
    which means,
    \begin{equation*}
        \hat{\theta}\left(\textbf{X}_{n}\right)\stackrel{p}{\rightarrow}\theta_{0}.
    \end{equation*}
\end{proof}

\paragraph{Asymptotic Normality of MLE}

\paragraph{Efficiency of MLE}

\subsection{Modified Likelihood Estimator}

Seek a modified likelihood function that depends on as few of the nuisance parameters as possible while sacrificing as little information as possible.

\subsubsection{Marginal Likelihood}

\subsubsection{Conditional Likelihood}

Let $\boldsymbol{\theta}=(\boldsymbol{\varphi},\boldsymbol{\lambda})$, where $\boldsymbol{\varphi}$ is the parameter vector of interest and $\boldsymbol{\lambda}$ is a vector of nuisance parameters. The conditional likelihood can be obtained as follows:
\begin{enumerate}
    \item Find the complete sufficient statistic $S_{\boldsymbol{\lambda}}$, respectively for $\boldsymbol{\lambda}$.
    \item  Construct the conditional log-likelihood
          \begin{equation}
              \ell_{c}=\log\left(f_{Y\mid S_{\boldsymbol{\lambda}}}\right)
          \end{equation}
          where $f_{Y\mid S_{\boldsymbol{\lambda}}}$ is the conditional distribution of the response $Y$ given $S_{\boldsymbol{\lambda}}$.
\end{enumerate}

\begin{remark}
    Two cases might occur, that, for fixed $\boldsymbol{\varphi}_{0}$, $S_{\boldsymbol{\lambda}}\left(\boldsymbol{\varphi}_{0}\right)$ depends on $\boldsymbol{\varphi}_{0}$; or $S_{\boldsymbol{\lambda}}\left(\boldsymbol{\varphi}_{0}\right)=S_{\boldsymbol{\lambda}}$ is independent of $\boldsymbol{\varphi}_{0}$.
    \begin{enumerate}
        \item Independent:
        \item Dependent:
    \end{enumerate}
\end{remark}

Suppose that the log-likelihood for $\boldsymbol{\theta}=\left(\boldsymbol{\varphi},\boldsymbol{\lambda}\right)$ can be written in the exponential family form
\begin{equation}
    \ell\left(\boldsymbol{\theta},\mathbf{y}\right)=\boldsymbol{\theta}^{\prime}\mathbf{s}-b\left(\boldsymbol{\theta}\right)
\end{equation}

Also, suppose $\ell\left(\boldsymbol{\theta},\mathbf{y}\right)$ has a decomposition of the form
\begin{equation}
    \ell\left(\boldsymbol{\theta},\mathbf{y}\right)=\boldsymbol{\varphi}^{\prime}\mathbf{s}_{1}+\boldsymbol{\lambda}^{\prime}\mathbf{s}_{2}-b(\boldsymbol{\varphi},\boldsymbol{\lambda})
\end{equation}

\begin{remark}
    The above decomposition can be achieved only if $\boldsymbol{\varphi}$ is a linear function of $\theta$. The choice of nuisance parameter $\lambda$ is arbitrary and the inferences regarding $\boldsymbol{\varphi}$ should be unaffected by the parameterization chosen for $\lambda$.
\end{remark}

The conditional likelihood of the data $\mathbf{Y}$ given $\mathbf{s}_{2}$ is
\begin{equation}
    \ell\left(\boldsymbol{\varphi}\mid\mathbf{s}_{2}\right)=\boldsymbol{\varphi}^{\prime}\mathbf{s}_{1}-b^{*}\left(\boldsymbol{\varphi},\boldsymbol{\lambda}\right)
\end{equation}
which is independent of the nuisance parameter and may be used for inferences regarding $\boldsymbol{\varphi}$.

\begin{example}
    $Y_{1}\sim P\left(\mu_{1}\right),Y_{2}\sim P\left(\mu_{2}\right)$ are independent. Suppose $\varphi=\log\left(\frac{\mu_{2}}{\mu_{1}}\right)=\log\left(\mu_{2}\right)-\log\left(\mu_{1}\right)$ is the parameter of interest and the nuisance parameter is
    \begin{enumerate}
        \item $\lambda_{1}=\log\left(\mu_{1}\right)$.
        \item
    \end{enumerate}
    Then, give the conditional log-likelihood for different nuisance parameter.
\end{example}

\begin{proof}
    \begin{enumerate}
        \item
              The log-likelihood function in the form of $\left(\varphi,\lambda\right)$ is
              \begin{equation*}
                  \begin{aligned}
                      \ell\left(\phi,\lambda_{1}\right)\propto & \log\left[e^{-\left(\mu_{1}+\mu_{2}\right)}\mu_{1}^{y_{1}}\mu_{2}^{y_{2}}\right]                              \\
                      =                                        & -\left(\mu_{1}+\mu_{2}\right)+y_{1}\log\left(\mu_{1}\right)+y_{2}\log\left(\mu_{2}\right)                     \\
                      =                                        & -\mu_{1}\left(1+\frac{\mu_{2}}{\mu_{1}}\right)+y_{1}\log\left(\mu_{1}\right)+y_{2}\log\left(\mu_{1}\right)    \\
                                                               & -y_{2}\left[\log\left(\mu_{1}\right)-\log\left(\mu_{2}\right)\right]                                          \\
                      =                                        & -\mathrm{e}^{\lambda_{1}}\left(1+\mathrm{e}^{\varphi}\right)+\left(y_{1}+y_{2}\right)\lambda_{1}-y_{2}\varphi \\
                      =                                        & s_{1}\varphi+s_{2}\lambda_{1}-b\left(\varphi,\lambda_{1}\right)
                  \end{aligned}
              \end{equation*}
              where $s_{1}=-y_{2},s_{2}=y_{1}+y_{2},b\left(\varphi,\lambda_{1}\right)=e^{\lambda_{1}}\left(1+e^{\varphi}\right)$.

              Then, the conditional distribution of $Y_{1},Y_{2}$ given $S_{2}=Y_{1}+Y_{2}$ is $b\left(S_{2},\frac{\mu_{1}}{\mu_{1}+\mu_{2}}\right)$, thus,
              \begin{equation*}
                  \begin{aligned}
                      \ell\left(\varphi\mid S_{2}=s_{2}\right)\propto & y_{1}\log\left(\frac{\mu_{1}}{\mu_{1}+\mu_{2}}\right)+y_{2}\log\left(\frac{\mu_{2}}{\mu_{1}+\mu_{2}}\right)          \\
                      =                                               & y_{1}\log\left(\frac{\mu_{1}}{\mu_{1}+\mu_{2}}\right)+y_{2}\log\left(\frac{\mu_{1}}{\mu_{1}+\mu_{2}}\right)          \\
                                                                      & -y_{2}\left[\log\left(\frac{\mu_{1}}{\mu_{1}+\mu_{2}}\right)-\log\left(\frac{\mu_{2}}{\mu_{1}+\mu_{2}}\right)\right] \\
                      =                                               & \left(y_{1}+y_{2}\right)\log\left(\frac{1}{1+e^{\varphi}}\right)-y_{2}\varphi                                        \\
                      =                                               & s_{1}\varphi-b^{*}\left(\varphi,s_{2}\right)
                  \end{aligned}
              \end{equation*}
              where $b^{*}\left(\varphi,s_{2}\right)=-s_{2}\log\left(\frac{1}{1+\varphi^{-1}}\right)$.
    \end{enumerate}
\end{proof}

\subsubsection{Profile Likelihood}

\subsubsection{Quasi Likelihood}

\section{Interval Estimation}

\subsection{Resampling}

\subsubsection{Jackknife}

Suppose the independent and identically distributed (i.i.d) sample $\boldsymbol{x}=\left(x_{1},x_{2},\ldots,x_{n}\right)^{\prime}$ from an unknown probability distribution $F$ on some probability space $\mathcal{X}$
\begin{equation}
    x_{i}\stackrel{\text{iid}}{\sim}F,\quad i=1,2,\ldots,n
\end{equation}
and a real-valued statistic $\hat{\theta}$ can be computed by applying some algorithm $s(\cdot)$ to $\mathbf{x}$, that,
\begin{equation}
    \hat{\theta}_{n}=s(\mathbf{x})
\end{equation}

Let $\mathbf{x}_{(i)}$ be the sample with $x_{i}$ removed,
\begin{equation}
    \mathbf{x}_{(i)}=\left(x_{1},x_{2},\ldots,x_{i-1},x_{i+1},\ldots,x_{n}\right)^{\prime}
\end{equation}
and denote the corresponding value of the statistic of interest as
\begin{equation}
    \hat{\theta}_{(i)}=s\left(\mathbf{x}_{(i)}\right)
\end{equation}

\paragraph{Bias of $\hat{\theta}$}

For almost all reasonable and practical estimates, we have \begin{equation}
    \operatorname{Bias}(\hat{\theta}_{n})=\operatorname{E}(\hat{\theta}_{n})-\theta\rightarrow 0,\quad\text{as }n\rightarrow\infty
\end{equation}
Then, it is reasonable to assume a power series of the type
\begin{equation}
    \operatorname{E}(\hat{\theta}_{n})=\theta+\frac{a_{1}}{n}+\frac{a_{2}}{n^{2}}+\frac{a_{3}}{n^{3}}+\ldots
\end{equation}
with some coefficients $\left\{a_{k}\right\}$. And we have
\begin{equation}
    \operatorname{E}(\hat{\theta}_{(i)})=\operatorname{E}(\hat{\theta}_{n-1})=\theta+\frac{a_{1}}{n-1}+\frac{a_{2}}{\left(n-1\right)^{2}}+\frac{a_{3}}{\left(n-1\right)^{3}}+\ldots
\end{equation}

For the sake of a smaller variance, we average all such estimates and let
\begin{equation}
    \hat{\theta}_{(\cdot)}=\frac{1}{n}\sum_{i=1}^{n}\hat{\theta}_{(i)}
\end{equation}
thus,
\begin{equation}
    \operatorname{E}(\hat{\theta}_{(\cdot)})=\theta+\frac{a_{1}}{n-1}+\frac{a_{2}}{\left(n-1\right)^{2}}+\frac{a_{3}}{\left(n-1\right)^{3}}+\ldots
\end{equation}

By equation and, we have
\begin{equation}
    (n-1)\operatorname{E}\left[\hat{\theta}_{(\cdot)}-\hat{\theta}_{n}\right]=\frac{a_{1}}{n}+\frac{a_{2}}{n^{2}}+\frac{a_{3}}{n^{3}}+\ldots=\operatorname{Bias}(\hat{\theta})
\end{equation}
Hence, we can get the jackknife estimate bias for $\hat{\theta}$ be
\begin{equation}
    \widehat{\operatorname{Bias}}_{\text{jack}}=(n-1)\left(\hat{\theta}_{(\cdot)}-\hat{\theta}_{n}\right)
\end{equation}

% Variance of the bias of $\hat{\theta}$

\begin{remark}
    It is easy to combine the averaged Jackknife estimator $\hat{\theta}_{(i)}$ with the original $\hat{\theta}$, to kill the main term in the bias of $\hat{\theta}$, thus,
    \begin{equation}
        \begin{aligned}
            \mathbf{E}\left[n\hat{\theta}_{n}-(n-1)\hat{\theta}_{(\mathbf{\cdot})}\right] & =\left[n\theta-(n-1)\theta\right]+\left[a_{1}-a_{1}\right]+\left[\frac{a_{2}}{n}-\frac{a_{2}}{n-1}\right]+\ldots \\
                                                                                          & =\theta+\frac{a_{2}}{n(n-1)}+\ldots=\theta+\frac{a_{2}}{n^{2}}+O\left(n^{-3}\right)
        \end{aligned}
    \end{equation}
    This removes the bias in the special case that the bias is $O\left(n^{-1}\right)$ and removes it to $O\left(n^{-2}\right)$ in other cases.
\end{remark}

\paragraph{Variance of $\hat{\theta}$}

The jackknife estimate of variance for $\hat{\theta}$ is
\begin{equation}
    \widehat{\operatorname{Var}}_{\text{jack}}=\frac{n-1}{n}\sum_{i=1}^{n}\left(\hat{\theta}_{(i)}-\hat{\theta}_{(\cdot)}\right)^{2},\quad\text{where }\hat{\theta}_{(\cdot)}=\frac{1}{n}\sum_{i=1}^{n}\hat{\theta}_{(i)}
\end{equation}

The jackknife method of estimation can fail if the statistic $\hat{\theta}_{\text{jack}}$ is not smooth. Smoothness
implies that relatively small changes to data values will cause only a small change in the
statistic.

\begin{example}[Sample Mean]

\end{example}

\begin{example}[Sample Correlation Coefficient]

\end{example}

\subsubsection{Bootstrap}

\paragraph{Nonparametric Bootstrap}

\paragraph{Bayesiam Bootstrap}

\paragraph{Smooth Bootstrap}

\paragraph{Parametric Bootstrap}

\paragraph{Block Bootstrap}

\section{Testing Hypotheses}