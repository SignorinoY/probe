
\chapter{The Delta Methods}

\section{The Delta Methods}

\begin{theorem}[Delta Method]
    Let $\{X_{n}\}$ be a sequence of random variables with
    \begin{equation}
        \sqrt{n}\left(X_{n}-\theta\right)\stackrel{d}{\rightarrow}N\left(0,\sigma^{2}\right)
    \end{equation}
    where $\theta$ and $\sigma$ are finite, then for any function $g$ with the property that $g'(\theta)$ exists and is non-zero valued,
    \begin{equation}
        \sqrt{n}\left[g\left(X_{n}\right)-g(\theta)\right] \stackrel{d}{\rightarrow}N\left(0,\sigma^{2}\cdot\left[g^{\prime}(\theta)\right]^{2}\right)
    \end{equation}
\end{theorem}

\begin{proof}

    \begin{enumerate}
        \item  Under the assumption that $g'(\theta)$ is continuous.

              Since, $g'(\theta)$ exists, with the first-order Taylor Approximation, that
              \begin{equation*}
                  g(X_n)=g(\theta)+g'(\tilde{\theta})(X_n-\theta)
              \end{equation*}
              where $\tilde{\theta}$ lies between $X_n$ and $\theta$.
              Since $X_n\stackrel{p}{\rightarrow}\theta$, and $|\tilde{\theta}-\theta|<|X_n-\theta|$, then
              \begin{equation*}
                  \tilde{\theta}\stackrel{p}{\rightarrow}\theta
              \end{equation*}
              Since $g'(\theta)$ is continuous, by Continuous Mapping Theorem (\ref{thm:continuous-mapping-theorem}),
              \begin{equation*}
                  g'(\tilde{\theta})\stackrel{p}{\rightarrow}g'(\theta)
              \end{equation*}
              and,
              \begin{equation*}
                  \sqrt{n}\left(g(X_n)-g(\theta)\right)=\sqrt{n}g'(\tilde{\theta})(X_n-\theta)
              \end{equation*}
              \begin{equation*}
                  \sqrt{n}\left(X_{n}-\theta\right)\stackrel{d}{\rightarrow}N\left(0,\sigma^{2}\right)
              \end{equation*}
              by Slutsky's Theorem (\ref{thm:slutsky-theorem}),
              \begin{equation*}
                  \sqrt{n}\left[g\left(X_{n}\right)-g(\theta)\right] \stackrel{d}{\rightarrow}N\left(0,\sigma^{2}\cdot\left[g^{\prime}(\theta)\right]^{2}\right)
              \end{equation*}
    \end{enumerate}
\end{proof}

\begin{theorem}[Second-order Delta Method]

\end{theorem}

\begin{remark}
    We can approximate the moments of a function $f(\cdot)$ of a random variable $X$ using Taylor expansions, provided that $f(\cdot)$ is sufficiently differentiable and that the moments of $X$ are finite. Suppose $\mu=\operatorname{E}\left(X\right)$, and $\sigma^{2}=\operatorname{Var}\left(X\right)$, wiht the Taylor expansions for the functions of random variables,
    \begin{equation}
        f\left(X\right)=f\left[\mu+\left(X-\mu\right)\right]\approx f\left(\mu\right)+f^{\prime}\left(\mu\right)\left(X-\mu\right)
    \end{equation}
    Thus,
    \begin{equation}
        \operatorname{E}\left[f\left(X\right)\right]\approx\operatorname{E}\left[f\left(\mu\right)\right],\quad\operatorname{Var}\left[f(X)\right]\approx\left[f^{\prime}\left(\mu\right)\right]^{2}\cdot\sigma^{2}
    \end{equation}

\end{remark}
